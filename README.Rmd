---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# Multiple Imputation by Super Learning (MISL)

[![Travis-CI Build Status](https://travis-ci.com/carpenitoThomas/misl.svg?token=u9TyfsxVjq6xxvc5h473&branch=master)](https://travis-ci.com/carpenitoThomas/misl)
[![Coverage Status](https://codecov.io/gh/carpenitoThomas/misl/branch/master/graph/badge.svg?token=C157LCBBJI)](https://codecov.io/gh/carpenitoThomas/misl)
[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)


The goal of MISL (Multiple Imputation by Super Learning) is to create multiply imputed datasets using the super learning framework. This package builds heavily off of the `sl3` and `mice` packages.

## Installation

You can install the released version of misl from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("misl")
```

And the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("carpenitoThomas/misl")
```
## Example

Here's an example with the nhanes data in which we use `misl()` imputation and then pool the results:

```{r}
library(misl)
library(mice)

misl_imp_2 <- misl(misl::nhanes, maxit = 5, m = 5, quiet = FALSE, con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_xgboost"), bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_xgboost"), cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))

plan(list(tweak(multisession, workers = 4), sequential))
misl_imp <- misl(misl::nhanes, maxit = 5, m = 4, quiet = FALSE,
                  con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_glmnet", "Lrnr_polspline"),
                  bin_method = c("Lrnr_mean", "Lrnr_earth", "Lrnr_glm_fast"),
                  cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))

mice_imp <- mice(misl::nhanes)

misl_modeling <- lapply(misl_imp, function(y){
  stats::lm(TotChol ~ Age + Weight + Height + Smoke100 + Education, data = y$datasets)
})

summary(mice::pool(misl_modeling), conf.int = TRUE)
```

We can also look at the traceplot of the imputations as well:

```{r}
traceplot(misl_imp)
```

This package also supports paralellization with the `future` package. One can choose to paralellize either the outside creation of datasets or the learners in the super learner library (or both!). The following snippet explains how this can be accomplished with four test-case scenarios (with an assumption that our computer has 8 cores):

```{r, echo = TRUE, eval= FALSE}
library(future)

# Sequential dataset processessing, Sequential super learning  (default)
plan(list(sequential,sequential))
seq_seq <- misl(nhanes)

# Sequential dataset processessing, paralell super learning (8) 
plan(list(sequential,tweak(multisession, workers = 8)))
seq_par <- misl(nhanes)

# Paralelle dataset processessing (8), sequential super learning 
plan(list(tweak(multisession, workers = 5), sequential))
par_seq <- misl(nhanes)

# paralell dataset processessing (4), paralell super learning (2) 
plan(list(tweak(multisession, workers = 4),tweak(multisession, workers = 2)))
par_par <- misl(nhanes)

# paralell dataset processing to ensure you don't overload your computer
plan(list(tweak(multisession, workers = availableCores() %/% 4),tweak(multisession, workers = 4)))
par_safe <- misl(nhanes)

```

Reminder, paralellizing code is not a silver bullet to automate making runtime processes faster. Make sure you have an understanding of the capacity of your computer. Further information about the topology of running code in paralell can be found in the future package.







