ifelse(is.na(dataset$Length), NA, dataset$Length)
value <- 1
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset$Length), NA, dataset$Length), maxmatch = 5, tol = 10000)
matches
matches$matches
matches$matches[1]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset$Length), NA, dataset$Length), maxmatch = 5, tol = 10000)
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset$Length), NA, dataset$Length), maxmatch = 5, tol = 10000)
ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
matches
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset$Length), NA, dataset$Length), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
#list_of_matches[value] <- dataset[[column]][sample(matches$matches)[1]]
}
hist(list_of_matches)
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset$Length), NA, dataset$Length), maxmatch = 5, tol = 10000)
#list_of_matches[value] <- ifelse(is.na(dataset$Length), NA, dataset$Length)[matches$matches[1]]
list_of_matches[value] <- dataset[[column]][sample(matches$matches)[1]]
}
hist(list_of_matches)
load_all()
load_all()
misl_imp <- misl(misl::abalone)
misl_imp <- misl(misl::abalone, quiet = FALSE)
traceplot(misl_imp)
dataset <- misl::abalone
m = 5
maxit = 5
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean")
missing_default = "sample"
m_loop <- 1
i_loop <- 1
# Initialize the return object (or, the dataframes that we want to return)
imputed_datasets <- vector("list", m)
# Initializes the trace plot (for inspection of imputations)
trace_plot <- expand.grid(statistic = c("mean", "sd"), value = NA, variable = colnames(dataset), m = m_loop, iteration = seq_along(1:maxit))
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- colnames(dataset)[colSums(is.na(dataset))!=0]
column <- "Length"
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
if(!quiet){print(paste("Imputing:", column))}
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# Here, we need to fill in the remaining empty cells and we do this with random sampling.
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(full_dataframe)){
full_dataframe[is.na(full_dataframe[[column_number]]), column_number] <-  impute_placeholders(full_dataframe, column_number, missing_default)
}
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations)
sl3_task <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar)
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binary = bin_method ,
continuous = con_method)
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
# We need to add a bit of code until a PR is accepted for the SL3 package for using bayesGLM (2/23/21)
if(learner == "SL.bayesglm"){
code.lm <- paste(learner, " <- sl3::Lrnr_pkg_SuperLearner$new(\"SL.ranger\")")
}else{
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
}
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
# We are now at the point where we can obtain predictions for matching candidates using X_miss
# We are only interested in those values for which our dataset[[column]] is missing, but we can make predictions on the entire dataset, that's OK!
dataset_copy <- dataset_master_copy
for(column_number in seq_along(dataset_copy)){
# If this is the first iteration then we're going to have missing values for some of our rows.
dataset_copy[is.na(dataset_copy[[column_number]]), column_number] <- impute_placeholders(dataset, column_number, missing_default)
}
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# But, we're going to follow the bootstrap matching method: https://stefvanbuuren.name/fimd/sec-cart.html#sec:cartoverview
# Which is interesting becuase it looks like our beta hat and beta dot are one in the same: https://stefvanbuuren.name/fimd/sec-categorical.html
predictions_task <- sl3::sl3_Task$new(dataset_copy, covariates = xvars, outcome = yvar)
predictions <- sl_stack_fit$predict(predictions_task)
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
predictions[dataset[[column]]]
dataset
column
predictions
length*
asl
length(predictions[dataset[[column]]])
length(predictions)
dataset[[column]]]
dataset[[column]]
length(ifelse(is.na(dataset[[column]]), NA, predictions))
View(ifelse(is.na(dataset[[column]]), NA, predictions))
View(predictions)
View(abalone$Length)
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
ifelse(is.na(dataset[[column]]), NA, predictions)
value <- 1
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
matches
ifelse(!is.na(dataset[[column]]), NA, predictions)
sum(is.na(ifelse(!is.na(dataset[[column]]), NA, predictions)))
sum(!is.na(ifelse(!is.na(dataset[[column]]), NA, predictions)))
colSums(is.na(dataset))
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = ifelse(!is.na(dataset[[column]]), NA, predictions), y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
}
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
}
ifelse(is.na(dataset[[column]]), NA, predictions)
matches
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
list_of_matches
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
}
list_of_matches
matches
value
predictions[4177]
predictions[1530]
ifelse(is.na(dataset[[column]]), NA, dataset[[column]])
View(ifelse(is.na(dataset[[column]]), NA, dataset[[column]]))
ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
value
predictions[4177]
matches
value <- 16
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
matches
predictions[16]
predictions[1859]
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
}
hist(list_of_matches)
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
}
hist(list_of_matches)
dataset_master_copy[[column]]<- ifelse(is.na(dataset[[column]]), list_of_matches, dataset[[column]])
hist(dataset_master_copy[[column]])
hist(dataset_master_copy[[column]][is.na(dataset_master_copy[[column]])])
hist(dataset_master_copy[[column]])
hist(dataset_master_copy[[column]][is.na(dataset[[column]])])
hist(dataset_master_copy[[column]][!is.na(dataset[[column]])])
hist(dataset[[column]][!is.na(dataset[[column]])])
load_all()
misl_imp <- misl(misl::abalone)
traceplot(misl_imp)
mice_imp <- mice(misl::abalone)
plot(mice_imp)
?mice
?any
?mice.impute.pmm
misl_modeling <- lapply(misl_imp, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
?pool
mice_modeling <- pool(mice_imp, exp = lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12))
mice_modeling <- pool(data = mice_imp, exp = lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12))
mice_modeling <- with(data = mice_imp, exp = lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12))
summary(mice::pool(misl_modeling), conf.int = TRUE)
print("******")
summary(pool(mice_modeling), conf.int = TRUE)
?min
min(c(1,2,3,4,3,3,3,3) - c(2,3,3,2,2,2,1,1,2,2,2,1))
min(c(10) - c(2,3,3,2,2,2,1,1,2,2,2,1))
min(c(10) - c(2,3,3,2,2,2,1,1,2,2,2,1),3)
install.packages("Rfast")
library("Rpath")
library("Rfast")
?rfast
?nth
x <- matrix( rnorm(100 * 100), ncol = 100 )
elems <- sample(1:100,100,TRUE)
colnth(x,elems)
rownth(x,elems)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213))
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), k = 2)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), k = 10)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 5)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 5, k = 1)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 5, k = 1:3)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 3)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 3, k = 1)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 3, k = 2)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 3, k = 3)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 3, k = 4)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 3, k = 10)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 1, k = 10)
nth(c(2,23,2,1,23,2,1,23,12,31,23,123,2,312,31,23,213), num.of.nths = 2, k = 10)
?sort
?which.maxn
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
dataset <- misl::abalone
m = 5
maxit = 5
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean"),
cat_method = c("Lrnr_mean")
missing_default = "sample",
missing_default = "sample"
quiet = TRUE
m_loop <- 1
i_loop <- 1
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- colnames(dataset)[colSums(is.na(dataset))!=0]
column <- "Length"
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# Here, we need to fill in the remaining empty cells and we do this with random sampling.
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(full_dataframe)){
full_dataframe[is.na(full_dataframe[[column_number]]), column_number] <-  impute_placeholders(full_dataframe, column_number, missing_default)
}
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations)
sl3_task <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar)
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binary = bin_method ,
continuous = con_method)
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
# We need to add a bit of code until a PR is accepted for the SL3 package for using bayesGLM (2/23/21)
if(learner == "SL.bayesglm"){
code.lm <- paste(learner, " <- sl3::Lrnr_pkg_SuperLearner$new(\"SL.ranger\")")
}else{
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
}
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
# We are now at the point where we can obtain predictions for matching candidates using X_miss
# We are only interested in those values for which our dataset[[column]] is missing, but we can make predictions on the entire dataset, that's OK!
dataset_copy <- dataset_master_copy
for(column_number in seq_along(dataset_copy)){
# If this is the first iteration then we're going to have missing values for some of our rows.
dataset_copy[is.na(dataset_copy[[column_number]]), column_number] <- impute_placeholders(dataset, column_number, missing_default)
}
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# But, we're going to follow the bootstrap matching method: https://stefvanbuuren.name/fimd/sec-cart.html#sec:cartoverview
# Which is interesting becuase it looks like our beta hat and beta dot are one in the same: https://stefvanbuuren.name/fimd/sec-categorical.html
predictions_task <- sl3::sl3_Task$new(dataset_copy, covariates = xvars, outcome = yvar)
predictions <- sl_stack_fit$predict(predictions_task)
outcome_type
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
View(predictions)
value <- 1
x <- predictions[value]
x
distance <- abs(predictions[value] - predictions)
View(distance)
sort(distance)
View(sort(distance))
row.names(predictions)
rownames(predictions)
predictions <- sl_stack_fit$predict(predictions_task)
rownames(predictions)
?sort
View(sort(distance, index.return = TRUE))
order(distance)
View(order(distance))
distance <- order(abs(predictions[value] - predictions))
distance
distance <- order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions)))
View(distance)
nrow(distance)
length(distance)
length(predictions)
distance <- order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions)))
distance
View(View(ifelse(is.na(dataset[[column]]), NA, predictions))))
View(ifelse(is.na(dataset[[column]]), NA, predictions))
View(ifelse(is.na(dataset[[column]]), NA, predictions))
colSums(is.na(dataset$Length))
colSums(is.na(dataset))
4177-339
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),5)
distance
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),2:6)
?head
distance <- order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions)))
distance
value <- 5
distance <- order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions)))
head(distance)
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),6)
distance
distance[-1]
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),6)[-1]
distance
value <- 1
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),6)[-1]
distance
value <- 16
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),6)[-1]
distance
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),6)
distance
distance
sample(distance,1)
sample(distance,1)
sample(distance,1)
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),5)
#matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[sample(distance,1)]
}
hist(list_of_matches)
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
}
system.time(list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
})
system.time({list_of_matches <- c()
for(value in seq_along(predictions)){
matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[matches$matches[1]]
}})
system.time({list_of_matches <- c()
for(value in seq_along(predictions)){
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),5)
#matches <- Hmisc::find.matches(x = predictions[value], y = ifelse(is.na(dataset[[column]]), NA, predictions), maxmatch = 5, tol = 10000)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[sample(distance,1)]
}})
?use_package
?use_this::use_package
?usethis::use_package()
misl_imp <- misl(misl::abalone)
misl_imp <- misl(misl::abalone, quiet = FALSE)
misl_imp <- misl(misl::abalone, quiet = FALSE)
load_all()
misl_imp <- misl(misl::abalone, quiet = FALSE)
restart()
library('devtools')
load_all()
misl_imp <- misl(misl::abalone, quiet = FALSE)
load_all()
misl_imp <- misl(misl::abalone, quiet = FALSE)
misl_modeling <- lapply(misl_imp, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
misl_imp <- misl(misl::abalone, maxit = 5, m = 5, quiet = FALSE,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_glmnet", "Lrnr_polspline"),
bin_method = c("Lrnr_mean", "Lrnr_earth", "Lrnr_glm_fast"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))
library("abind")
setwd("/Users/thomascarpenito/Documents/Northeastern/Dissertation/MISL Paper/simulation_results/")
load("truth.Rdata")
true <- truth_model$coefficients[4]
organize_results <- function(combined){
RB <- rowMeans(combined[,, "Estimate"]) - true
PB <- 100 * abs((rowMeans(combined[,, "Estimate"]) - true)/ true)
CR <- rowMeans(combined[,, "2.5 %"] < true & true < combined[,, "97.5 %"])
AW <- rowMeans(combined[,, "97.5 %"] - combined[,, "2.5 %"])
results <- data.frame(RB, PB, CR, AW)
return(results)
}
mcar_20 <- list.files("mcar/mcar/", pattern = "mcar_20..\\.Rdata", full.names = TRUE)
mcar_40 <- list.files("mcar/mcar/", pattern = "mcar_40..\\.Rdata", full.names = TRUE)
mcar_60 <- list.files("mcar/mcar/", pattern = "mcar_60..\\.Rdata", full.names = TRUE)
for(file in c(mcar_20, mcar_40,mcar_60 )){
load(file)
}
combined_mcar_20 <- abind(mcar_20_1, mcar_20_2, mcar_20_3, mcar_20_4, mcar_20_5, along = 2)
combined_mcar_40 <- abind(mcar_40_1, mcar_40_2, mcar_40_3, mcar_40_4, mcar_40_5, along = 2)
combined_mcar_60 <- abind(mcar_60_1, mcar_60_2, mcar_60_3, mcar_60_4, mcar_60_5, along = 2)
mcar_20 <- organize_results(combined_mcar_20)
mcar_40 <- organize_results(combined_mcar_40)
mcar_60 <- organize_results(combined_mcar_60)
buildmcar_40
mcar_40
mcar_60
library("devtools")
load_all()
System.time(misl(misl::abalone))
system.time(misl(misl::abalone))
system.time(misl(misl::abalone, quiet = FALSE))
traceplot(misl_imp)
library("future")
plan(list(tweak(multisession, workers = 5), sequential))
system.time(misl_imp <- misl(misl::abalone, quiet = FALSE))
traceplot(misl_imp)
mice_imp <- mice(misl::abalone)
library("mice")
mice_imp <- mice(misl::abalone)
plot(mice_imp)
plan(list(tweak(multisession, workers = 5), sequential))
system.time(misl_imp <- misl(misl::abalone, quiet = FALSE, con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_xgboost")))
traceplot(misl_imp)
system.time(misl_imp <- misl(misl::abalone, quiet = FALSE, con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_xgboost"), cat_method = "Lrnr_independent_binomial", "Lrnr_mean"), bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_xgboost"))
system.time(misl_imp <- misl(misl::abalone, quiet = FALSE, con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_xgboost"), cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"), bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_xgboost"))
system.time(misl_imp <- misl(misl::abalone, quiet = FALSE, con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_xgboost"), cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"), bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_xgboost")))
