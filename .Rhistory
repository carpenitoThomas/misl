View(predictions_task_boot_dot$time)
sl_stack_fit_boot_dot$predict
?sl_stack_fit_boot_dot$predict
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_task_boot_dot
sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
sl_stack_fit_boot_dot$predict(predictions_task_boot_dot, prob = FALSE)
predictions_task_boot_dot
predictions_task_boot_dot$outcome_type
predictions_task_boot_dot$.__enclos_env__
predictions_task_boot_dot$Y
predictions_task_boot_dot$folds
predictions_task_boot_dot$subset_task()
predictions_task_boot_dot$Y
temp
sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
predictions_boot_dot[1]
sum(predictions_boot_dot[1])
library('devtools')
load_all()
dataset <- abalone
m = 5
maxit = 5
seed = NA
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial")
ignore_predictors = NA
quiet = FALSE
delta_cat = 1
delta_var = "Sex"
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
column_order
column <- Sex
column <- "Sex"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# This is a quick fix for the sensitivity analysis that will require further thought
# Essentially, we are chceking to see if this is the value we need to augment or not.
# If it's not, then no changes happen
delta_adj = FALSE
if(!is.na(delta_var)){
if(column == delta_var){
delta_adj = TRUE
}
}
delta_adj
delta_var
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = .2 )
dataset$offset <- .2
dataset_master_copy$offset <- .2
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = offset )
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = "offset" )
predictions_task_boot_dot
dataset_master_copy$offset <- 100
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = "offset" )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
post <- sl3::unpack_predictions(predictions_boot_dot)
View(post)
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type)
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
View(sl3::unpack_predictions(predictions_boot_dot))
?glm
dataset <- abalone
load_all
library('devtools')
load_all
load_all()
dataset <- abalone
load_all()
m = 5
maxit = 5
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial")
ignore_predictors = NA
quiet = TRUE
delta_con = 0
delta_cat = 2
delta_var <- "Sex"
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
column <- "Sex"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# This is a quick fix for the sensitivity analysis that will require further thought
# Essentially, we are chceking to see if this is the value we need to augment or not.
# If it's not, then no changes happen
delta_adj = FALSE
if(!is.na(delta_var)){
if(column == delta_var){
delta_adj = TRUE
}
}
delta_adj
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# For categorical data we follow advice suggested by Van Buuren:
# https://github.com/cran/mice/blob/master/R/mice.impute.polyreg.R
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
post <- sl3::unpack_predictions(predictions_boot_dot)
post
delta_adj
max.col(post)
sampled_delta_col <- sample(ncol(post),1)
sampled_delta_col
sampled_delta_col <- max.col(post)
sampled_delta_col
View(sampled_delta_col)
View(post)
View(abalone)
View(post[, sampled_delta_col] / delta_cat)
View(post[, sampled_delta_col])
View(sampled_delta_col)
View(post[,1])
View(post[, sampled_delta_col])
?max.col
str(post)
post[,2]
olnames(post)[max.col(post)]
colnames(post)[max.col(post)]
sampled_delta_col <- colnames(post)[max.col(post)]
View(post[, sampled_delta_col])
a1 <- post / apply(post, 1, max)
View(a1)
apply(post, 1, max)
?lapply
a1 <- apply(post, 1, function(x){
max <- which.max(x)
print(max)
})
a1 <- apply(post, 1, function(x){
max <- which.max()
print(max)
})
a1 <- apply(post, 1, function(x){
max <- which.max(x)
})
a1 <- apply(post, 1, function(x){
max <- which.max(x)
print(max)
})
colnames(post)[max.col(post)]
dim(post[, sampled_delta_col])
View(post[, sampled_delta_col])
sampled_delta_col <- [max.col(post)]
sampled_delta_col <- max.col(post)
sampled_delta_col
?sweep
a <- sweep(post, sampled_delta_col, delta_cat, "/"  )
rowmax <- apply(post, 1, max)
rowmax
View(rowmax)
?rowwise
sampled_delta_col <- max.col(post)
sampled_delta_col
delta_cat
?scale
a <- apply(post, 1, function(x){
which.max(x) + 100
})
a
View(a)
View(post)
?pmax
a <- apply(X=post, MARGIN=1, FUN=max)
pmax()
a
a <- pmax(post)
a
View(df$max<-apply(X=df, MARGIN=1, FUN=max))
View(a)
a <- pmax(post)
a$to <- pmax(post)
View(a)
a <- pmax(post)
str(a)
install.packages("rfast")
install.packages("Rfast")
library('Rfast')
?rowMaxs()
rowMaxs(post)
a <- rowMaxs(post)
a
max.col(post)
sampled_delta_col <- max.col(post)
Viewsampled_delta_col
sampled_delta_col <- max.col(post)
sampled_delta_col
View(t(post[, sampled_delta_col]))
View(post[, sampled_delta_col])
View(post[1:nrow(post), sampled_delta_col])
View(sampled_delta_col)
sampled_delta_col <- which.max(post)
sampled_delta_col
sampled_delta_col <- max.col(post)
sampled_delta_col
post[,2]
post[1,2]
for(row_num in seq_along(nrow(post))){
print(row_num)
}
nrow(post)
for(row_num in seq_along(1:nrow(post))){
print(row_num)
}
a <- post
View(a)
for(row_num in seq_along(1:nrow(post))){
post[row_num, sampled_delta_col[row_num]] <- post[row_num, sampled_delta_col[row_num]] / delta_cat
}
View(post)
posty <- post
post <- t(scale(t(post), center = FALSE, scale = colSums(t(post))))
View(posty)
View(post)
View(a)
library(misl)
set.seed(123)
misl_imp <- misl(abalone, maxit = 2, m = 2, quiet = TRUE,
con_method = c("Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger"),
bin_method = c("Lrnr_earth", "Lrnr_glm_fast", "Lrnr_ranger"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_ranger"))
misl_modeling <- lapply(misl_imp, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
summary(mice::pool(misl_modeling), conf.int = TRUE)
summary(mice::pool(misl_modeling), conf.int = TRUE)
colSums(is.na(abalone))
colSums(is.na(abalone)) / colSums(abalone)
colSums(is.na(abalone)) / nrow(abalone)
nrow(abalone)
library("mice")
mice::plot(abalone)
md.pattern(abalone)
?md.pattern
md.pattern(abalone, rotate.names = TRUE)
print(md.pattern(abalone, rotate.names = TRUE))
md.pattern(abalone, rotate.names = TRUE)
md.pattern(abalone, rotate.names = TRUE)
lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = abalone)
summary(lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = abalone))
library("devtools")
use_r("misl_mnar")
load_all()
build(vignettes = FALSE)
?misl
document()
?misl
library('devtools')
load_all()
build(vignettes = FALSE)
document()
?misl
install()
library("misl")
misl
?misl
library("devtools")
load_all()
install()
library('misl')
?misl
load_all()
?misl
install(reload = TRUE)
?misl
?misl
document9)
document()
?misl
?misl_mnar
load_all()
document()
install(reload = TRUE)
build(vignettes = FALSE)
install(reload = TRUE)
?misl
?misl
library("misl")
?misl
?misl_mnar
load_all()
?misl
?misl_mnar
library(misl)
set.seed(123)
misl_imp <- misl(abalone, maxit = 5, m = 5, quiet = FALSE)
colnames(abalone)
misl_imp_mnar <- misl_mnar(abalone, maxit = 5, m = 5, quiet = FALSE, delta_cat = 3, delta_var = "Length")
misl_modeling <- lapply(misl_imp, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
misl_modeling_mnar <- lapply(misl_imp_mnar, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling_mnar), conf.int = TRUE)
plot(misl_imp)
sl3::sl3_list_learners()
sl3::sl3_list_learners("categorical")
sl3_list_learners("categorical")
library('sl3')
sl3_list_learners("binomial")
sl3::sl3_list_learners("binomial")
library('devtools')
load_all()
?misl
sl3_list_learners("binomial")
sl3::sl3_list_learners("binomial")
nhanes <- misl::nhanes
head(nhanes)
str(nhanes)
round(colSums(is.na(nhanes)) / nrow(nhanes), 2)
mice::md.pattern(nhanes, rotate.names = TRUE)
sl3::sl3_list_learners("categorical")
library(sl3)
sl3::sl3_list_learners("categorical")
sl3::sl3_list_learners("binomial")
sdtr(nhanes)
str(nhanes)
build()
vignette(misl)
?vignette
vignette(package = "misl")
load_all()
vignette(misl)
library(misl)
library(tidyverse)
library(tidyverse)
library(mice)
library(sl3)
set.seed(12345)
nhanes <- misl::nhanes
head(nhanes)
round(colSums(is.na(nhanes)) / nrow(nhanes), 2)
mice::md.pattern(nhanes, rotate.names = TRUE)
sl3::sl3_list_learners("continuous")
sl3::sl3_list_learners("binomial")
sl3::sl3_list_learners("categorical")
misl_imputations <- misl(dataset = nhanes,
m = 1,
maxit = 5,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth"),
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_ranger"),
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial"),
quiet = TRUE
)
plot(misl_imputations)
misl_modeling <- lapply(misl_imputations, function(y){
stats::lm(TotChol ~ Age + Weight + Height + Smoke100 + Education, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
misl_imputations_mnar_1 <- misl_mnar(dataset = nhanes,
m = 1,
maxit = 5,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth"),
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_ranger"),
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial"),
quiet = TRUE,
delta_var = "Smoke100",
delta_cat = 2
)
misl_modeling_mnar <- lapply(misl_imputations_mnar_1, function(y){
stats::lm(TotChol ~ Age + Weight + Height + Smoke100 + Education, data = y$datasets)
})
summary(mice::pool(misl_modeling_mnar), conf.int = TRUE)
vignette(misl)
vignette("misl")
load_all()
vignette(package = "misl")
vignette("misl")
install()
