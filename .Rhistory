colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shuck_Weight", "Viscera_Weight", "Shell_Weight", "Rings")
abalone <- abalone %>%
dplyr::mutate(Age = Rings + 1.5,
Sex = as.factor(Sex),
Older_12 = as.integer(Age > 12)
) %>%
dplyr::select(-c(Rings, Age, Shuck_Weight, Viscera_Weight, Shell_Weight))
mypatterns <- expand.grid(Sex = 0:1, Length = 0:1, Diameter = 0:1, Height = 0:1, Whole_Weight_Pred = 0:1, Older_12 = 0:1)
mypatterns <- mypatterns[sample(1:nrow(mypatterns), replace = FALSE, 8),]
mypatterns <- mypatterns[rowSums(mypatterns) != 0,]
amputed_abalone <- mice::ampute(abalone,
mech = "MAR",
prop = .50)
abalone <- as_tibble(amputed_abalone$amp)
str(abalone)
colSums(is.na(abalone))
abalone2 <- abalone %>%
mutate(
Sex = as.factor(Sex)
)
str(abalone2)
load("/Users/thomascarpenito/Documents/Programming/misl/data-raw/abalone.rdata")
# add the column names
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shuck_Weight", "Viscera_Weight", "Shell_Weight", "Rings")
str(abalone$Sex)
str(abalone)
mypatterns <- expand.grid(Sex = 1, Length = 1, Diameter = 1, Height = 1, Whole_Weight_Pred = 0, Older_12 = 1)
amputed_abalone <- mice::ampute(abalone,
mech = "MAR",
prop = .50)
abalone <- as_tibble(amputed_abalone$amp)
colSums(is.na(abalone))
load("/Users/thomascarpenito/Documents/Programming/misl/data-raw/abalone.rdata")
# add the column names
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shuck_Weight", "Viscera_Weight", "Shell_Weight", "Rings")
abalone <- abalone %>%
dplyr::mutate(Age = Rings + 1.5,
Sex = as.factor(Sex),
Older_12 = as.integer(Age > 12)
) %>%
dplyr::select(-c(Rings, Age, Shuck_Weight, Viscera_Weight, Shell_Weight))
colSums(is.na(abalone))
mypatterns <- expand.grid(Sex = 1, Length = 1, Diameter = 1, Height = 1, Whole_Weight_Pred = 0, Older_12 = 1)
amputed_abalone <- mice::ampute(abalone,
mech = "MAR",
prop = .50)
abalone <- as_tibble(amputed_abalone$amp)
colSums(is.na(abalone))
load("/Users/thomascarpenito/Documents/Programming/misl/data-raw/abalone.rdata")
# add the column names
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shuck_Weight", "Viscera_Weight", "Shell_Weight", "Rings")
abalone <- abalone %>%
dplyr::mutate(Age = Rings + 1.5,
Sex = as.factor(Sex),
Older_12 = as.integer(Age > 12)
) %>%
dplyr::select(-c(Rings, Age, Shuck_Weight, Viscera_Weight, Shell_Weight))
mypatterns <- expand.grid(Sex = 1, Length = 1, Diameter = 1, Height = 1, Whole_Weight_Pred = 0, Older_12 = 1)
amputed_abalone <- mice::ampute(abalone,
mech = "MAR",
patterns = mypatterns,
prop = .50)
colSums(is.na(amputed_abalone))
abalone <- as_tibble(amputed_abalone$amp)
colSums(is.na(abalone))
load("/Users/thomascarpenito/Documents/Programming/misl/data-raw/abalone.rdata")
# add the column names
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shuck_Weight", "Viscera_Weight", "Shell_Weight", "Rings")
abalone <- abalone %>%
dplyr::mutate(Age = Rings + 1.5,
Sex = as.factor(Sex),
Older_12 = as.integer(Age > 12)
) %>%
dplyr::select(-c(Rings, Age, Shuck_Weight, Viscera_Weight, Shell_Weight))
mypatterns <- expand.grid(Sex = 1, Length = 0:1, Diameter = 0:1, Height = 0:1, Whole_Weight_Pred = 0:1, Older_12 = 0:1)
mypatterns <- mypatterns[sample(1:nrow(mypatterns), replace = FALSE, 8),]
mypatterns <- mypatterns[rowSums(mypatterns) != 0,]
amputed_abalone <- mice::ampute(abalone,
mech = "MAR",
patterns = mypatterns,
prop = .50)
abalone <- as_tibble(amputed_abalone$amp)
colSums(is.na(abalone))
load("/Users/thomascarpenito/Documents/Programming/misl/data-raw/abalone.rdata")
# add the column names
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shuck_Weight", "Viscera_Weight", "Shell_Weight", "Rings")
abalone <- abalone %>%
dplyr::mutate(Age = Rings + 1.5,
Sex = as.factor(Sex),
Older_12 = as.integer(Age > 12)
) %>%
dplyr::select(-c(Rings, Age, Shuck_Weight, Viscera_Weight, Shell_Weight))
str(abalone)
mypatterns <- expand.grid(Sex = 1, Length = 0:1, Diameter = 0:1, Height = 0:1, Whole_Weight_Pred = 0:1, Older_12 = 0:1)
mypatterns <- mypatterns[sample(1:nrow(mypatterns), replace = FALSE, 8),]
mypatterns <- mypatterns[rowSums(mypatterns) != 0,]
amputed_abalone <- mice::ampute(abalone,
mech = "MAR",
patterns = mypatterns,
prop = .50)
abalone2 <- as_tibble(amputed_abalone$amp)
View(abalone2)
View(abalone)
abalone2 <- abalone2 %>%
mutate(
Sex = as.factor(Sex, levels("F", "I", "M"))
)
abalone2 <- abalone2 %>%
mutate(
Sex = factor(Sex, levels("F", "I", "M"))
)
abalone2 <- abalone2 %>%
mutate(
Sex = factor(Sex, levels = c("F", "I", "M"))
)
str()
str(abalone2)
abalone2 <- as_tibble(amputed_abalone$amp)
str(abalone2)
abalone2 <- abalone2 %>%
mutate(
Sex = factor(Sex, levels = c("F", "I", "M"))
)
View(abalone2)
abalone2 <- as_tibble(amputed_abalone$amp)
str(abalone2)
abalone2 <- as_tibble(amputed_abalone$amp)
abalone2 <- abalone2 %>%
mutate(
Sex = factor(Sex, labels = c("F", "I", "M"), levels = c(1,2,3))
)
load("/Users/thomascarpenito/Documents/Programming/misl/data-raw/abalone.rdata")
library(data.table)
library(tidyverse)
library(usethis)
library(mice)
set.seed(1234)
# add the column names
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shuck_Weight", "Viscera_Weight", "Shell_Weight", "Rings")
abalone <- abalone %>%
dplyr::mutate(Age = Rings + 1.5,
Sex = as.factor(Sex),
Older_12 = as.integer(Age > 12)
) %>%
dplyr::select(-c(Rings, Age, Shuck_Weight, Viscera_Weight, Shell_Weight))
mypatterns <- expand.grid(Sex = 0:1, Length = 0:1, Diameter = 0:1, Height = 0:1, Whole_Weight_Pred = 0:1, Older_12 = 0:1)
mypatterns <- mypatterns[sample(1:nrow(mypatterns), replace = FALSE, 8),]
mypatterns <- mypatterns[rowSums(mypatterns) != 0,]
amputed_abalone <- mice::ampute(abalone,
mech = "MAR",
patterns = mypatterns,
prop = .50)
abalone2 <- as_tibble(amputed_abalone$amp)
abalone2 <- abalone2 %>%
mutate(
Sex = factor(Sex, labels = c("F", "I", "M"), levels = c(1,2,3))
)
abalone <- as_tibble(amputed_abalone$amp)
abalone <- abalone %>%
mutate(
Sex = factor(Sex, labels = c("F", "I", "M"), levels = c(1,2,3))
)
str(abalone)
colSums(is.na(abalone))
colSums(is.na(abalone))/nrow(abalone)
write_csv(abalone, "data-raw/abalone.csv")
usethis::use_data(abalone, overwrite = TRUE, compress = 'xz')
load_all()
build(vignettes = FALSE)
build_readme()
library('devtools')
load_all()
abalone
sampled_abalone <- sample_n(abalone, 500)
sampled_abalone <- samplen(abalone, 500)
sampled_abalone <- sample(abalone, 500)
dataset <- abalone
m = 5
maxit = 5
seed = NA
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial")
ignore_predictors = NA
missing_default = "sample"
quiet = FALSE
# Initialize the return object (or, the dataframes that we want to return)
imputed_datasets <- vector("list", m)
m_loop <- 1
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
i_loop <- 1
column_order
column <- "Sex"
if(!quiet){print(paste("Imputing:", column))}
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
outcome_type
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# For categorical data we follow advice suggested by Van Buuren:
# https://github.com/cran/mice/blob/master/R/mice.impute.polyreg.R
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
uniform_values
post <- sl3::unpack_predictions(predictions_boot_dot)
post
View(post)
rowSums(post)
Viiew(draws)
draws <- uniform_values > apply(post, 1, cumsum)
View(draws)
cumsum
uniform_values
a <- NA
if(a){print("Tommy")}
norm(c(.2,.3,.4))
?scale
scale(c(.2,.3,.5))
str(post)
ncol(post)
sample(3,1)
sample(3,1)
sample(3,1)
sample(3,1)
sample(3,1)
sample(3,1)
sample(3,1)
sample(3,1)
sample(3,1)
sampled_delta_col[,1]
post[,1]
View(post[,1])
delta = 2
delta <- 2
View(sampled_delta_col[, sampled_delta_col] / delta)
sampled_delta_col <- sample(ncol(post),1)
View(sampled_delta_col[, sampled_delta_col] / delta)
View(post[, sampled_delta_col] / delta)
sampled_delta_col
View(post[, sampled_delta_col] / delta_cat)
delta_cat <- 2
View(post[, sampled_delta_col] / delta_cat)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
scale(post, center = FALSE, scale = rowSums(post))
scale(matrix(post), center = FALSE, scale = rowSums(matrix(post)))
View(post)
?scale
matrix_post <- as.matrix(post)
str(matrix_post)
scale(matrix_post, center = FALSE, scale = rowSums(matrix_post))
View(matrix_post)
scale(t(matrix_post), center = FALSE, scale = colSums(t(matrix_post)))
View(scale(t(matrix_post), center = FALSE, scale = colSums(t(matrix_post))))
View(t(scale(t(matrix_post), center = FALSE, scale = colSums(t(matrix_post)))))
.23605529  + 	0.28898132 + 0.47496339
post <- sl3::unpack_predictions(predictions_boot_dot)
t(scale(t(post), center = FALSE, scale = colSums(t(post))))
View(t(scale(t(post), center = FALSE, scale = colSums(t(post)))))
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
View(t(scale(t(post), center = FALSE, scale = colSums(t(post)))))
post <- sl3::unpack_predictions(predictions_boot_dot)
sampled_delta_col <- 2
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
View(post)
View(t(scale(t(post), center = FALSE, scale = colSums(t(post)))))
delta_cat <- 0
post <- sl3::unpack_predictions(predictions_boot_dot)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
View(post)
delta_cat <- 1
post <- sl3::unpack_predictions(predictions_boot_dot)
View(sl3::unpack_predictions(predictions_boot_dot))
post <- sl3::unpack_predictions(predictions_boot_dot)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
post <- t(scale(t(post), center = FALSE, scale = colSums(t(post))))
View(post)
delta_cat <- 2
post <- sl3::unpack_predictions(predictions_boot_dot)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
post <- t(scale(t(post), center = FALSE, scale = colSums(t(post))))
rowSums(post)
column_order
column <- "Older_12"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
yvar
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
outcome_type
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
View(predictions_boot_dot)
delta_cat
View(predictions_boot_dot / delta_cat)
column_order
column <- "Height"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
yvar
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
outcome_type
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# This step only needs to compute if the outcome is continuous, saving some time
if(outcome_type == "continuous"){
sl_train_full_hat <- sl3::delayed_learner_train(sl, sl3_task_full_hat)
sl_sched_full_hat <- delayed::Scheduler$new(sl_train_full_hat, delayed::FutureJob)
sl_stack_fit_full_hat <- sl_sched_full_hat$compute()
predictions_task_full_hat <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_full_hat <- sl_stack_fit_full_hat$predict(predictions_task_full_hat)
}
View(predictions_boot_dot)
hist(abalone$Height)
delta_con
delta_con <- 0
# We can add a bit of augemntation here for the sensitivity analysis
# By default, this should not affect results as we will be adding 0, otherwise, augment the imputations
predictions_boot_dot <- predictions_boot_dot + delta_con
build(vignettes = FALSE)
load_all()
?misl
load_all()
?misl
build(vignettes = FALSE)
load_all()
library(devtools)
library("devtools")
load_all()
?misl
