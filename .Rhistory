a <- sweep(post, sampled_delta_col, delta_cat, "/"  )
rowmax <- apply(post, 1, max)
rowmax
View(rowmax)
?rowwise
sampled_delta_col <- max.col(post)
sampled_delta_col
delta_cat
?scale
a <- apply(post, 1, function(x){
which.max(x) + 100
})
a
View(a)
View(post)
?pmax
a <- apply(X=post, MARGIN=1, FUN=max)
pmax()
a
a <- pmax(post)
a
View(df$max<-apply(X=df, MARGIN=1, FUN=max))
View(a)
a <- pmax(post)
a$to <- pmax(post)
View(a)
a <- pmax(post)
str(a)
install.packages("rfast")
install.packages("Rfast")
library('Rfast')
?rowMaxs()
rowMaxs(post)
a <- rowMaxs(post)
a
max.col(post)
sampled_delta_col <- max.col(post)
Viewsampled_delta_col
sampled_delta_col <- max.col(post)
sampled_delta_col
View(t(post[, sampled_delta_col]))
View(post[, sampled_delta_col])
View(post[1:nrow(post), sampled_delta_col])
View(sampled_delta_col)
sampled_delta_col <- which.max(post)
sampled_delta_col
sampled_delta_col <- max.col(post)
sampled_delta_col
post[,2]
post[1,2]
for(row_num in seq_along(nrow(post))){
print(row_num)
}
nrow(post)
for(row_num in seq_along(1:nrow(post))){
print(row_num)
}
a <- post
View(a)
for(row_num in seq_along(1:nrow(post))){
post[row_num, sampled_delta_col[row_num]] <- post[row_num, sampled_delta_col[row_num]] / delta_cat
}
View(post)
posty <- post
post <- t(scale(t(post), center = FALSE, scale = colSums(t(post))))
View(posty)
View(post)
View(a)
library(misl)
set.seed(123)
misl_imp <- misl(abalone, maxit = 2, m = 2, quiet = TRUE,
con_method = c("Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger"),
bin_method = c("Lrnr_earth", "Lrnr_glm_fast", "Lrnr_ranger"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_ranger"))
misl_modeling <- lapply(misl_imp, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
summary(mice::pool(misl_modeling), conf.int = TRUE)
summary(mice::pool(misl_modeling), conf.int = TRUE)
colSums(is.na(abalone))
colSums(is.na(abalone)) / colSums(abalone)
colSums(is.na(abalone)) / nrow(abalone)
nrow(abalone)
library("mice")
mice::plot(abalone)
md.pattern(abalone)
?md.pattern
md.pattern(abalone, rotate.names = TRUE)
print(md.pattern(abalone, rotate.names = TRUE))
md.pattern(abalone, rotate.names = TRUE)
md.pattern(abalone, rotate.names = TRUE)
lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = abalone)
summary(lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = abalone))
library("devtools")
use_r("misl_mnar")
load_all()
build(vignettes = FALSE)
?misl
document()
?misl
library('devtools')
load_all()
build(vignettes = FALSE)
document()
?misl
install()
library("misl")
misl
?misl
library("devtools")
load_all()
install()
library('misl')
?misl
load_all()
?misl
install(reload = TRUE)
?misl
?misl
document9)
document()
?misl
?misl_mnar
load_all()
document()
install(reload = TRUE)
build(vignettes = FALSE)
install(reload = TRUE)
?misl
?misl
library("misl")
?misl
?misl_mnar
load_all()
?misl
?misl_mnar
library(misl)
set.seed(123)
misl_imp <- misl(abalone, maxit = 5, m = 5, quiet = FALSE)
colnames(abalone)
misl_imp_mnar <- misl_mnar(abalone, maxit = 5, m = 5, quiet = FALSE, delta_cat = 3, delta_var = "Length")
misl_modeling <- lapply(misl_imp, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
misl_modeling_mnar <- lapply(misl_imp_mnar, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling_mnar), conf.int = TRUE)
plot(misl_imp)
sl3::sl3_list_learners()
sl3::sl3_list_learners("categorical")
sl3_list_learners("categorical")
library('sl3')
sl3_list_learners("binomial")
sl3::sl3_list_learners("binomial")
library('devtools')
load_all()
?misl
sl3_list_learners("binomial")
sl3::sl3_list_learners("binomial")
nhanes <- misl::nhanes
head(nhanes)
str(nhanes)
round(colSums(is.na(nhanes)) / nrow(nhanes), 2)
mice::md.pattern(nhanes, rotate.names = TRUE)
sl3::sl3_list_learners("categorical")
library(sl3)
sl3::sl3_list_learners("categorical")
sl3::sl3_list_learners("binomial")
sdtr(nhanes)
str(nhanes)
build()
vignette(misl)
?vignette
vignette(package = "misl")
load_all()
vignette(misl)
library(misl)
library(tidyverse)
library(tidyverse)
library(mice)
library(sl3)
set.seed(12345)
nhanes <- misl::nhanes
head(nhanes)
round(colSums(is.na(nhanes)) / nrow(nhanes), 2)
mice::md.pattern(nhanes, rotate.names = TRUE)
sl3::sl3_list_learners("continuous")
sl3::sl3_list_learners("binomial")
sl3::sl3_list_learners("categorical")
misl_imputations <- misl(dataset = nhanes,
m = 1,
maxit = 5,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth"),
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_ranger"),
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial"),
quiet = TRUE
)
plot(misl_imputations)
misl_modeling <- lapply(misl_imputations, function(y){
stats::lm(TotChol ~ Age + Weight + Height + Smoke100 + Education, data = y$datasets)
})
summary(mice::pool(misl_modeling), conf.int = TRUE)
misl_imputations_mnar_1 <- misl_mnar(dataset = nhanes,
m = 1,
maxit = 5,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth"),
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_ranger"),
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial"),
quiet = TRUE,
delta_var = "Smoke100",
delta_cat = 2
)
misl_modeling_mnar <- lapply(misl_imputations_mnar_1, function(y){
stats::lm(TotChol ~ Age + Weight + Height + Smoke100 + Education, data = y$datasets)
})
summary(mice::pool(misl_modeling_mnar), conf.int = TRUE)
vignette(misl)
vignette("misl")
load_all()
vignette(package = "misl")
vignette("misl")
install()
load_all()
library('devtools')
load_all()
vignette("misl")
vignette(misl)
vignette()
devtools::build()
install()
libary("misl")
library("misl")
vignette("misl")
vignette(misl)
vignette(mice)
vignette("mice")
library("mice")
vignette("mice")
vignette(mice)
?mice
browseVignettes()
vignette()
build()
install()
load_all()
library("devtools")
load_all()
mcar <- lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = abalone)
summary(mcar)
confint(mcar)
str(dataset)
str(data)
?check_dataset
dataset <- data
m <- 1
dataset
dataset <- read.csv("/Users/thomascarpenito/Documents/Northeastern/Dissertation/Pesticides/server_code/merged_analyte_hsd_wide.csv", stringsAsFactors = TRUE)
# TODO: Builds out more checks to ensure the MISL algorithm can run properly
check_dataset(dataset)
library('devtools')
load_all()
# TODO: Builds out more checks to ensure the MISL algorithm can run properly
check_dataset(dataset)
# Initialize the return object (or, the dataframes that we want to return)
imputed_datasets <- vector("list", m)
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
column_order
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
column_number
sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
dataset[[column_number]][!is.na(dataset[[column_number]])]
dataset[[column_number]]
View(dataset[[71]])
View(dataset)
library('devtools')
load_all()
load("../../../Desktop/merged_analyte_hsd_wide.Rdata")
misl_imp_test <- misl(merged_analyte_hsd_wide,
con_method = c("Lrnr_glm_fast", "Lrnr_earth"),
bin_method = c("Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_rpart"),
ignore_predictors = c("motherID", "pretermBirth", "race"),
maxit = 1,
m = 1, quiet = FALSE)
misl_imp_test <- misl(merged_analyte_hsd_wide,
con_method = c("Lrnr_glm_fast", "Lrnr_earth"),
bin_method = c("Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_rpart"),
ignore_predictors = c("motherID", "pretermBirth", "race"),
maxit = 2,
m = 1, quiet = FALSE)
dataset <- merged_analyte_hsd_wide
m = 5
maxit = 5
seed = NA
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean")
ignore_predictors = NA
quiet <- FALSE
# TODO: Builds out more checks to ensure the MISL algorithm can run properly
check_dataset(dataset)
# Initialize the return object (or, the dataframes that we want to return)
imputed_datasets <- vector("list", m)
# This apply function defines each of the imputed m datasets
# The future.seed = TRUE argument ensures parallel safe random numbers are generated. See the Future package for more information https://cran.r-project.org/web/packages/future.apply/future.apply.pdf
imputed_datasets <- future.apply::future_lapply(future.stdout = NA, future.seed=TRUE, seq_along(1:m), function(m_loop){
# Do users want to know which dataset they are imputing?
if(!quiet){print(paste("Imputing dataset:", m_loop))}
# Initializes the trace plot (for inspection of imputations)
trace_plot <- expand.grid(statistic = c("mean", "sd"), value = NA, variable = colnames(dataset), m = m_loop, iteration = seq_along(1:maxit))
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
# Next, we begin the iterations within each dataset.
for(i_loop in seq_along(1:maxit)){
# Provide an option for if messages should be output
if(!quiet){print(paste("Imputing iteration:", i_loop))}
# Begin the iteration column by column
for(column in column_order){
if(!quiet){print(paste("Imputing:", column))}
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# We can begin defining our impuation model or, super learning
# More information on super learner can be found in the SL3 Package: https://github.com/tlverse/sl3
# Information on other models can be found: https://stefvanbuuren.name/fimd/how-to-generate-multiple-imputations.html
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
# We are now at the point where we can obtain predictions for matching candidates using X_miss
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# Original PMM uses type 1 matching, so that's what we are going to use
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# This step only needs to compute if the outcome is continuous, saving some time
if(outcome_type == "continuous"){
sl_train_full_hat <- sl3::delayed_learner_train(sl, sl3_task_full_hat)
sl_sched_full_hat <- delayed::Scheduler$new(sl_train_full_hat, delayed::FutureJob)
sl_stack_fit_full_hat <- sl_sched_full_hat$compute()
predictions_task_full_hat <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_full_hat <- sl_stack_fit_full_hat$predict(predictions_task_full_hat)
}
# Here we can begin imputation depending on the data type
if(outcome_type == "binomial"){
# Imputation for binary variables can be found from the following resources:
# https://stefvanbuuren.name/fimd/sec-categorical.html#def:binary
# https://github.com/cran/mice/blob/master/R/mice.impute.logreg.R
uniform_values <- runif(length(predictions_boot_dot))
predicted_values <- as.integer(uniform_values <= predictions_boot_dot)
dataset_master_copy[[column]] <- ifelse(is.na(dataset[[column]]), predicted_values, dataset[[column]])
}else if(outcome_type == "continuous"){
predictions_boot_dot <- predictions_boot_dot
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them - there are a lot of ways to do matching
# https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# Note, there are considerable slow-downs with our matching here and should be improved for efficiency
list_of_matches <- c()
for(value in seq_along(predictions_boot_dot)){
distance <- head(order(abs(predictions_boot_dot[value] - ifelse(is.na(dataset[[column]]), NA, predictions_full_hat))),5)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[sample(distance,1)]
}
dataset_master_copy[[column]]<- ifelse(is.na(dataset[[column]]), list_of_matches, dataset[[column]])
}else if(outcome_type== "categorical"){
# For categorical data we follow advice suggested by Van Buuren:
# https://github.com/cran/mice/blob/master/R/mice.impute.polyreg.R
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
post <- sl3::unpack_predictions(predictions_boot_dot)
draws <- uniform_values > apply(post, 1, cumsum)
idx <- 1 + apply(draws, 2, sum)
predicted_values <- levels(dataset[[column]])[idx]
dataset_master_copy[[column]] <-  factor(ifelse(is.na(dataset[[column]]), predicted_values, as.character(dataset[[column]])), levels = levels(dataset[[column]]))
}
# Append to the trace plot only if a numeric column
# A trace plot for categorical variable is not entirely meaningful... but imputations should be checked for plausibility
if(outcome_type != "categorical" & sum(is.na(dataset[[column]])) > 0){
trace_plot$value[trace_plot$variable == column & trace_plot$m == m_loop & trace_plot$iteration == i_loop & trace_plot$statistic == "mean"] <- mean(dataset_master_copy[[column]][is.na(dataset[[column]])])
trace_plot$value[trace_plot$variable == column & trace_plot$m == m_loop & trace_plot$iteration == i_loop & trace_plot$statistic == "sd"] <- sd(dataset_master_copy[[column]][is.na(dataset[[column]])])
}
}
}
# After all columns are imputed, we can save the dataset and trace plot for recall later
return_object <- list(datasets = dataset_master_copy, trace = trace_plot)
return_object
})
# TODO: Builds out more checks to ensure the MISL algorithm can run properly
check_dataset(dataset)
# Initialize the return object (or, the dataframes that we want to return)
imputed_datasets <- vector("list", m)
# Do users want to know which dataset they are imputing?
if(!quiet){print(paste("Imputing dataset:", m_loop))}
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
column_order
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
colnames(dataset)
column <- "concentration_1_DEE"
if(!quiet){print(paste("Imputing:", column))}
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
outcome_type
table(dataset[[yvar]])
levels(as.factor(dataset[[yvar]])))
levels(as.factor(dataset[[yvar]]))
levels(as.factor(x)) == c("0", "1")
levels(as.factor(dataset[[yvar]])) == c("0", "1")
sum(levels(as.factor(dataset[[yvar]])) == c("0", "1")) == 2
colnames(dataset)
sum(levels(as.factor(dataset[["female"]])) == c("0", "1")) == 2
library("devtools")
?install_github
misl::check_datatype()
misl::check_datatype
