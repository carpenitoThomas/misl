set.seed(123)
misl_imp_type_1 <- misl(abalone, maxit = 2, m = 5, quiet = FALSE,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_glmnet", "Lrnr_polspline"),
bin_method = c("Lrnr_mean", "Lrnr_earth", "Lrnr_glm_fast"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))
misl_modeling1 <- lapply(misl_imp_type_1, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
misl_modeling2 <- lapply(misl_imp_type_2, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling2), conf.int = TRUE)
summary(mice::pool(misl_modeling1), conf.int = TRUE)
a <- summary(mice::pool(misl_modeling1), conf.int = TRUE)
summary(mice::pool(misl_modeling1), conf.int = TRUE)["97.5 %"]
summary(mice::pool(misl_modeling2), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling2), conf.int = TRUE)["2.5 %"]
summary(mice::pool(misl_modeling1), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling1), conf.int = TRUE)["2.5 %"]
View(head(abalone))
colSums(is.na(abalone))
colSums(is.na(abalone)) / nrow(abalone)
mice_imputations <- mice(abalone)
mice_pool <- with(mice_imputations, exp = lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12))
mice_summary <- summary(mice::pool(misl_pool), conf.int = TRUE))
mice_summary <- summary(mice::pool(misl_pool), conf.int = TRUE)
mice_summary <- summary(mice::pool(mice_pool), conf.int = TRUE)
mice_summary
summary(mice::pool(misl_modeling2), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling2), conf.int = TRUE)["2.5 %"]
summary(mice::pool(misl_modeling1), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling1), conf.int = TRUE)["2.5 %"]
summary(mice::pool(mice_pool), conf.int = TRUE)["97.5 %"] - summary(mice::pool(mice_pool), conf.int = TRUE)["2.5 %"]
library(misl)
set.seed(123)
dataset <- abalone
m = 5
maxit = 5
seed = NA
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean")
ignore_predictors = NA
# Initializes the trace plot (for inspection of imputations)
trace_plot <- expand.grid(statistic = c("mean", "sd"), value = NA, variable = colnames(dataset), m = m_loop, iteration = seq_along(1:maxit))
m_loop <- 1
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- colnames(dataset)[colSums(is.na(dataset))!=0]
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe.
# You can see in this intialize function: https://github.com/amices/mice/blob/46171f911af7c7c668b4bffc3976f5669436bafd/R/initialize.imp.R
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
i_loop <- 1
column_order
column
column <- "Older_12"
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
full_dataframe
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice...
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
library('devtools')
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
load_all()
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
outcome_type
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different array types. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on both bootstrao and full_dataframes
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
sl_train_full_hat <- sl3::delayed_learner_train(sl, sl3_task_full_hat)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_sched_full_hat <- delayed::Scheduler$new(sl_train_full_hat, delayed::FutureJob)
# This step actually only needs to compute if the outcome is continuous, saving some time:
if(outcome_type == "continuous"){
sl_stack_fit_full_hat <- sl_sched_full_hat$compute()
}
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
# This step actually only needs to compute if the outcome is continuous, saving some time:
if(outcome_type == "continuous"){
sl_train_full_hat <- sl3::delayed_learner_train(sl, sl3_task_full_hat)
sl_sched_full_hat <- delayed::Scheduler$new(sl_train_full_hat, delayed::FutureJob)
sl_stack_fit_full_hat <- sl_sched_full_hat$compute()
predictions_task_full_hat <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_full_hat <- sl_stack_fit_full_hat$predict(predictions_task_full_hat)
}
outcome_type
# We don't actually just make draws from a binomial distribution... but rather compare draws to that from a uniform distribution
# I think this is what was responsible for poor imputations before.
#https://stefvanbuuren.name/fimd/sec-categorical.html#def:binary
predicted_values <- stats::rbinom(length(dataset_master_copy[[column]]), 1, predictions_boot_dot)
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# We don't actually just make draws from a binomial distribution... but rather compare draws to that from a uniform distribution
# I think this is what was responsible for poor imputations before.
#https://stefvanbuuren.name/fimd/sec-categorical.html#def:binary
predicted_values <- stats::rbinom(length(dataset_master_copy[[column]]), 1, predictions_boot_dot)
predicted_values
predictions_boot_dot
runif(nrow(predictions_boot_dot))
runif(length(predictions_boot_dot))
# We don't actually just make draws from a binomial distribution... but rather compare draws to that from a uniform distribution
# I think this is what was responsible for poor imputations before.
#https://stefvanbuuren.name/fimd/sec-categorical.html#def:binary
uniform_values <- runif(length(predictions_boot_dot))
View(uniform_values)
?runif
predicted_values <- predictions_boot_dot <= predictions_boot_dot
View(predicted_values)
predicted_values <- as.integer(predictions_boot_dot <= predictions_boot_dot)
View(predicted_values)
predicted_values <- as.integer(uniform_values <= predictions_boot_dot)
View(predicted_values)
predicted_values_2 <- stats::rbinom(length(dataset_master_copy[[column]]), 1, predictions_boot_dot)
table(predicted_values_2)
table(predicted_values)
str(predicted_values)
str(predicted_values_2)
column <- "Sex"
colnames(dataset)
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice...
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different array types. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on both bootstrao and full_dataframes
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# This step actually only needs to compute if the outcome is continuous, saving some time:
if(outcome_type == "continuous"){
sl_train_full_hat <- sl3::delayed_learner_train(sl, sl3_task_full_hat)
sl_sched_full_hat <- delayed::Scheduler$new(sl_train_full_hat, delayed::FutureJob)
sl_stack_fit_full_hat <- sl_sched_full_hat$compute()
predictions_task_full_hat <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_full_hat <- sl_stack_fit_full_hat$predict(predictions_task_full_hat)
}
outcome_type
?cumsum
predictions_boot_dot
l3::unpack_predictions(predictions_boot_dot
)
sl3::unpack_predictions(predictions_boot_dot)
sl3::unpack_predictions(predictions_boot_dot)
apply(sl3::unpack_predictions(predictions_boot_dot), 1, cumsum)
draws <- apply(sl3::unpack_predictions(predictions_boot_dot), 1, cumsum)
idx <- 1 + apply(draws, 2, sum)
idx
?polyreg
?mice.impute.polyreg
post <- sl3::unpack_predictions(predictions_boot_dot)
is.vector(post)
post <- matrix(post, nrow = 1, ncol = length(post))
post
draws <- un > apply(post, 1, cumsum)
?runif()
rep(runif(length(sl3::unpack_predictions(predictions_boot_dot)), each = 3)
)
un <- rep(runif(length(sl3::unpack_predictions(predictions_boot_dot)), each = 3))
un <- rep(runif(10), each = 3))
un <- rep(runif(10)), each = 3)
rep(runif(2), each = nc)
rep(runif(2), each = 3)
uniform_values <- rep(runif(length(predictions_boot_dot)), each = 3)
uniform_values
levels(dataset[[column]])
length(levels(dataset[[column]]))
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
View(uniform_values)
draws <- uniform_values > apply(post, 1, cumsum)
draws
View(uniform_values)
View(draws)
idx <- 1 + apply(draws, 2, sum)
View(idx)
post <- predictions_boot_dot
uniform_values > apply(post, 1, cumsum
)
post <- predictions_boot_dot
post <- matrix(c(1 - post, post), ncol = 2)
post <- matrix(post, nrow = 1, ncol = length(post))
is.vector(post)
draws <- uniform_values > apply(post, 1, cumsum)
# https://github.com/cran/mice/blob/master/R/mice.impute.polyreg.R
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
post <- predictions_boot_dot
post <- matrix(post, nrow = 1, ncol = length(post))
draws <- uniform_values > apply(post, 1, cumsum)
4177 * 3
predictions_boot_dot
post <- sl3::unpack_predictions(predictions_boot_dot)
post
post <- sl3::unpack_predictions(predictions_boot_dot)
post <- matrix(post, nrow = 1, ncol = length(post))
post
draws <- uniform_values > apply(post, 1, cumsum)
draws
apply(post, 1, cumsum)
post <- sl3::unpack_predictions(predictions_boot_dot)
draws <- uniform_values > apply(post, 1, cumsum)
draws
View(draws)
idx <- 1 + apply(draws, 2, sum)
idx
# https://github.com/cran/mice/blob/master/R/mice.impute.polyreg.R
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
post <- sl3::unpack_predictions(predictions_boot_dot)
#post <- matrix(post, nrow = 1, ncol = length(post))
draws <- uniform_values > apply(post, 1, cumsum)
idx <- 1 + apply(draws, 2, sum)
idx
apply(draws, 2, sum)
predicted_values <- Hmisc::rMultinom(sl3::unpack_predictions(predictions_boot_dot),1)
predicted_values
levels(dataset[[column]])[idx]
predicted_values_old <- Hmisc::rMultinom(sl3::unpack_predictions(predictions_boot_dot),1)
predicted_values_new <- levels(dataset[[column]])[idx]
predicted_values_new
table(predicted_values_new)
table(predicted_values_old)
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
post <- sl3::unpack_predictions(predictions_boot_dot)
#post <- matrix(post, nrow = 1, ncol = length(post))
draws <- uniform_values > apply(post, 1, cumsum)
idx <- 1 + apply(draws, 2, sum)
predicted_values_new <- levels(dataset[[column]])[idx]
predicted_values_old <- Hmisc::rMultinom(sl3::unpack_predictions(predictions_boot_dot),1)
table(predicted_values_old)
table(predicted_values_new)
str(predicted_values_new)
load_all()
library('devtools')
load_all()
library(misl)
set.seed(123)
misl_imp <- misl(abalone, maxit = 5, m = 5, quiet = FALSE,
con_method = c("Lrnr_mean", "Lrnr_glm_fast"),
bin_method = c("Lrnr_mean", "Lrnr_earth", "Lrnr_glm_fast"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))
load_all()
misl_imp <- misl(abalone, maxit = 5, m = 5, quiet = FALSE,
con_method = c("Lrnr_mean", "Lrnr_glm_fast"),
bin_method = c("Lrnr_mean", "Lrnr_earth", "Lrnr_glm_fast"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))
library('devtools')
load_all()
dataset <- abalone
colnames(dataset)[colSums(is.na(dataset))!=0]
sample(colnmaes(dataset))
sample(colnames(dataset))
sample(colnames(dataset))
sample(colnames(dataset))
sample(colnames(dataset))
sample(colnames(dataset))
sample(colnames(dataset))
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
column_order
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
column_order
library('devtools')
build(vignettes = FALSE)
library('devtools')
load_all()
dataset <- abalone
column <- "sex"
m = 5
maxit = 5
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean")
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe.
# You can see in this intialize function: https://github.com/amices/mice/blob/46171f911af7c7c668b4bffc3976f5669436bafd/R/initialize.imp.R
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
column
column <- "Sex"
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice...
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
ignore_predictors = NA
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different array types. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on both bootstrao and full_dataframes
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# This step actually only needs to compute if the outcome is continuous, saving some time:
if(outcome_type == "continuous"){
sl_train_full_hat <- sl3::delayed_learner_train(sl, sl3_task_full_hat)
sl_sched_full_hat <- delayed::Scheduler$new(sl_train_full_hat, delayed::FutureJob)
sl_stack_fit_full_hat <- sl_sched_full_hat$compute()
predictions_task_full_hat <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_full_hat <- sl_stack_fit_full_hat$predict(predictions_task_full_hat)
}
outcome_type
# https://github.com/cran/mice/blob/master/R/mice.impute.polyreg.R
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
uniform_values
View(uniform_values)
length(uniform_values)
12531/3
nrow(dataset)
post <- sl3::unpack_predictions(predictions_boot_dot)
View(post)
rowSums(post)
apply(post, 1, cumsum)
View(apply(post, 1, cumsum))
?cumsum
?apply
cumsum(c(1,2,3))
0.3062238 + 0.3310669
uniform_values > apply(post, 1, cumsum)
View(uniform_values > apply(post, 1, cumsum))
nrow(uniform_values)
length(uniform_values)
10 < apply(post, 1, cumsum)
View(10 < apply(post, 1, cumsum))
View(c(0,1,2,10) < apply(post, 1, cumsum))
idx <- 1 + apply(draws, 2, sum)
uniform_values <- rep(runif(length(predictions_boot_dot)), each = length(levels(dataset[[column]])))
post <- sl3::unpack_predictions(predictions_boot_dot)
draws <- uniform_values > apply(post, 1, cumsum)
idx <- 1 + apply(draws, 2, sum)
View(idx)
View(draws)
View(post)
View(uniform_values)
length(post)
c(1,2,3,4,5,6) < c(3, 2)
rowSums(as.numeric(draws))
rowSums(as.numeric(draws[3,]))
draws[3, ]
table(draws[3, ])
library("mice")
?mice
?ampute
library("mice")
?mice
