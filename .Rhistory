learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
learners
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
sl
# THROW AWAY
sl <- sl3::Lrnr_sl$new(Stack, Lrnr_glm_fast)
stack
# THROW AWAY
sl <- sl3::Lrnr_sl$new(Lrnr_glm_fast)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
sl <- sl3::Lrnr_sl$new(Lrnr_ranger)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
sl <- sl3::Lrnr_sl$new(Lrnr_earth)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
sl
learner_list
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
learner_list <- c( "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
sl_train
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_sched
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
sl_stack_fit
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
sl_sched
sl_sched$compute_step()
sl_sched$next_ready_task
sl_stack_fit <- sl_sched$compute()
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
sl$train()
stack_fit <- learner_stack$train(sl3_task)
stack_fit <- sl$train(sl3_task)
sl_stack_fit <- sl_sched$compute()
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
sl
learner_stack_code
# THROW AWAY
learner_list <- c("Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_earth", "Lrnr_glm_fast", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
learner_list <- c("Lrnr_ranger", "Lrnr_glm_fast", "Lrnr_mean", "Lrnr_earth")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
learner_list <- c("Lrnr_ranger", "Lrnr_glm_fast", "Lrnr_mean", "Lrnr_earth", "Lrnr_mean")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
sl_stack_fit
# THROW AWAY
learner_list <- c("Lrnr_ranger", "Lrnr_glm_fast", "Lrnr_mean", "Lrnr_earth", "Lrnr_ranger")
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
sl_stack_fit
library("misl")
?rbino
?rbinom
library("misl")
?misl
dataset <- nhanes
View(nhanes)
m = 5
maxit = 5
seed = NA
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger", "Lrnr_svm")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial", "Lrnr_ranger", "Lrnr_svm")
imputed_datasets <- vector("list", m)
m_loop <- 1
# Do users want to know which dataset they are imputing?
if(!quiet){print(paste("Imputing dataset:", m_loop))}
quiet <- FALSE
# Do users want to know which dataset they are imputing?
if(!quiet){print(paste("Imputing dataset:", m_loop))}
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- colnames(dataset)[colSums(is.na(dataset))!=0]
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe.
# You can see in this intialize function: https://github.com/amices/mice/blob/46171f911af7c7c668b4bffc3976f5669436bafd/R/initialize.imp.R
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  impute_placeholders(dataset_master_copy, column_number, missing_default)
}
i_loop <- 1
column_order
column <- "Smoke100"
if(!quiet){print(paste("Imputing:", column))}
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
ignore_predictors <- NA
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
load_all()
library('devtools')
load_all()
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
column_order
outcome_type
# First, define the task using our bootstrap_sample (this helps with variability in imputations)
sl3_task <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different array types. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# But, we're going to follow the bootstrap matching method: https://stefvanbuuren.name/fimd/sec-cart.html#sec:cartoverview
# Which is interesting becuase it looks like our beta hat and beta dot are one in the same: https://stefvanbuuren.name/fimd/sec-categorical.html
predictions_task <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions <- sl_stack_fit$predict(predictions_task)
predictions
predicted_values <- stats::rbinom(length(dataset_master_copy[[column]]), 1, predictions)
predicted_values
library('devtools')
load_all()
library("misl")
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger", "Lrnr_svm", "Lrnr_mean")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger", "Lrnr_mean")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial", "Lrnr_ranger", "Lrnr_svm", "Lrnr_mean")
dataset <- nhanes
View(dataset)
m = 5
maxit = 5
ignore_predictors = NA
quiet <- FALSE
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- colnames(dataset)[colSums(is.na(dataset))!=0]
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe.
# You can see in this intialize function: https://github.com/amices/mice/blob/46171f911af7c7c668b4bffc3976f5669436bafd/R/initialize.imp.R
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  impute_placeholders(dataset_master_copy, column_number, missing_default)
}
i_loop <- 1
column_order
column <- "Education"
if(!quiet){print(paste("Imputing:", column))}
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations)
sl3_task <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different array types. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
outcome_type
re_assign_cat_learners
learner_list
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = !quiet)
sl_stack_fit <- sl_sched$compute()
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# But, we're going to follow the bootstrap matching method: https://stefvanbuuren.name/fimd/sec-cart.html#sec:cartoverview
# Which is interesting becuase it looks like our beta hat and beta dot are one in the same: https://stefvanbuuren.name/fimd/sec-categorical.html
predictions_task <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions <- sl_stack_fit$predict(predictions_task)
predictions
outcome_type
predicted_values <- Hmisc::rMultinom(sl3::unpack_predictions(predictions),1)
predicted_values
sl3::unpack_predictions(predictions)
predicted_values <- Hmisc::rMultinom(sl3::unpack_predictions(predictions),1)
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
nrow(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
sum(Hmisc::rMultinom(sl3::unpack_predictions(predictions),1) == Hmisc::rMultinom(sl3::unpack_predictions(predictions),1))
library("mice")
?mice
library(misl)
library('devtools')
misl_imp_type_2 <- misl(abalone, maxit = 2, m = 5, quiet = FALSE,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_glmnet", "Lrnr_polspline"),
bin_method = c("Lrnr_mean", "Lrnr_earth", "Lrnr_glm_fast"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))
load_all()
set.seed(123)
misl_imp_type_1 <- misl(abalone, maxit = 2, m = 5, quiet = FALSE,
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_glmnet", "Lrnr_polspline"),
bin_method = c("Lrnr_mean", "Lrnr_earth", "Lrnr_glm_fast"),
cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))
misl_modeling1 <- lapply(misl_imp_type_1, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
misl_modeling2 <- lapply(misl_imp_type_2, function(y){
stats::lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12, data = y$datasets)
})
summary(mice::pool(misl_modeling2), conf.int = TRUE)
summary(mice::pool(misl_modeling1), conf.int = TRUE)
a <- summary(mice::pool(misl_modeling1), conf.int = TRUE)
summary(mice::pool(misl_modeling1), conf.int = TRUE)["97.5 %"]
summary(mice::pool(misl_modeling2), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling2), conf.int = TRUE)["2.5 %"]
summary(mice::pool(misl_modeling1), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling1), conf.int = TRUE)["2.5 %"]
View(head(abalone))
colSums(is.na(abalone))
colSums(is.na(abalone)) / nrow(abalone)
mice_imputations <- mice(abalone)
mice_pool <- with(mice_imputations, exp = lm(Whole_Weight ~ Sex + Length + Diameter + Height + Older_12))
mice_summary <- summary(mice::pool(misl_pool), conf.int = TRUE))
mice_summary <- summary(mice::pool(misl_pool), conf.int = TRUE)
mice_summary <- summary(mice::pool(mice_pool), conf.int = TRUE)
mice_summary
summary(mice::pool(misl_modeling2), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling2), conf.int = TRUE)["2.5 %"]
summary(mice::pool(misl_modeling1), conf.int = TRUE)["97.5 %"] - summary(mice::pool(misl_modeling1), conf.int = TRUE)["2.5 %"]
summary(mice::pool(mice_pool), conf.int = TRUE)["97.5 %"] - summary(mice::pool(mice_pool), conf.int = TRUE)["2.5 %"]
