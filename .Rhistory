# If the column was binary, we should round it
if(length(levels(as.factor(yvar))) == 2){
#dataset_master_copy[[column]] <- dataset_master_copy[[column]] > .5
#dataset_master_copy[[column]] <- rbinom(length(dataset_master_copy[[column]]), 1, prediction_summary$pred)
}
# Append to the trace plot
trace_plot[trace_plot_row_counter,] <- c(mean(dataset_master_copy[[column]][missing_yvar]), sd(dataset_master_copy[[column]][missing_yvar]), column, m, iteration)
# increase the trace_plot_row_counter
trace_plot_row_counter <- trace_plot_row_counter + 1
}
new_imputed_dataset <- dataset_master_copy
}
imputed_datasets <- append(imputed_datasets, list(new_imputed_dataset))
}
object <- list(datasets = imputed_datasets, trace = trace_plot)
return(list(object))
}
completed_nhanes <- misl(nhanes_incomplete)
completed_nhanes
View(completed_nhanes)
misl <- function(dataset){
trace_plot_row_counter <- 1
imputed_datasets <- c()
trace_plot <- data.frame(mean_value = NA, sd_value = NA, variable = NA, m = NA, iteration = NA)
for(m in 1:5){
print(paste("M ", m))
# To make this a multiple imputation, we will do this five times
# Note, here we prioritize by amount of available data
# Columns with MORE information (LESS missing data) are imputed first
columns <- colnames(md.pattern(dataset, plot = FALSE)[,1:length(dataset)])
dataset_master_copy <- dataset
# Begins the interations
for(iteration in 1:10){
#print(paste("Iteration ", iteration))
for(column in columns){
#print(column)
if(iteration ==1){
full_dataframe <- dataset_master_copy[!is.na(dataset_master_copy[[column]]), ]
}else{
full_dataframe <- new_imputed_dataset
#missing_yvar <- is.na(dataset_master_copy[[column]])
#full_dataframe <- dataset_master_copy[!missing_yvar, ]
}
yvar <- full_dataframe[[column]]
xvars <- full_dataframe[ , -which(names(full_dataframe) %in% c(column)), drop = FALSE]
# Keep track of the missing y_values
missing_yvar <- is.na(dataset[[column]])
# If the previous imputation did not impute values... then we will catch with mean and mode
for(i in 1:ncol(xvars)){
if(length(levels(as.factor(xvars[,i]))) == 2){
xvars[is.na(xvars[,i]), i] <-  Mode(xvars[,i])
}else{
xvars[is.na(xvars[,i]), i] <-  mean(xvars[,i], na.rm = TRUE)
}
}
# Note, sometimes the SuperLearner assigns 0 weight - this is has been requested to be turned into a warning.
if(length(levels(as.factor(yvar))) == 100){
# This is going to fail if the factor is two levels but isn't 0-1.
sl_lib = c( "SL.glm", "SL.lda", "SL.mean")
superlearner_column <-  try(SuperLearner(Y = yvar, X = xvars ,SL.library = sl_lib, family = binomial(), method = "method.NNLS"), silent = TRUE)
flag = ifelse(inherits(superlearner_column, "try-error"), TRUE, FALSE)
}else{
sl_lib = c("SL.glm", "SL.earth", "SL.loess", "SL.mean")
superlearner_column <- try(SuperLearner(Y = yvar, X = xvars ,SL.library = sl_lib, family = gaussian(), method = "method.NNLS"), silent = TRUE)
flag = ifelse(inherits(superlearner_column, "try-error"), TRUE, FALSE)
}
# This is useful for printing out the weights of the super learner per iteration
if(!flag){
#print(superlearner_column$coef)
}
# Make predictions on the dataset
# First we need to grab the original dataset and set the columns to either the mean or mode for the first iteration
if(iteration == 1){
dataset_copy <- dataset_master_copy
for(i in 1:ncol(dataset_copy)){
if(length(levels(as.factor(dataset_copy[,i]))) == 2){
dataset_copy[is.na(dataset_copy[,i]), i] <-  Mode(dataset_copy[,i])
}else{
dataset_copy[is.na(dataset_copy[,i]), i] <-  mean(dataset_copy[,i], na.rm = TRUE)
}
}
}else{
dataset_copy <- full_dataframe
}
new_x <- dataset_copy[ , -which(names(dataset_copy) %in% c(column)), drop = FALSE]
# We need to add the exception catcher so we can still make predictions
# So, if Superlearner fails we'll just use the mean
# Also new, we add a bit of noise to our imputations.
if(flag){
dataset_master_copy[[column]]<- ifelse(is.na(dataset[[column]]), mean(dataset[[column]], na.rm = TRUE), dataset[[column]])
}else{
prediction_summary <- predict(superlearner_column, newdata = new_x, onlySL = TRUE)
if(length(levels(as.factor(yvar))) == 2){
#dataset_master_copy[[column]] <- ifelse(missing_yvar, prediction_summary$pred, dataset[[column]])
predicted_values <- rbinom(length(dataset_master_copy[[column]]), 1, prediction_summary$pred)
dataset_master_copy[[column]] <- ifelse(missing_yvar, predicted_values, dataset[[column]])
}else{
dataset_master_copy[[column]]<- ifelse(missing_yvar, prediction_summary$pred + rnorm(n = length(prediction_summary$pred), mean = 0, sd = sd(prediction_summary$pred) ), dataset[[column]])
}
}
# If the column was binary, we should round it
if(length(levels(as.factor(yvar))) == 2){
#dataset_master_copy[[column]] <- dataset_master_copy[[column]] > .5
#dataset_master_copy[[column]] <- rbinom(length(dataset_master_copy[[column]]), 1, prediction_summary$pred)
}
# Append to the trace plot
trace_plot[trace_plot_row_counter,] <- c(mean(dataset_master_copy[[column]][missing_yvar]), sd(dataset_master_copy[[column]][missing_yvar]), column, m, iteration)
# increase the trace_plot_row_counter
trace_plot_row_counter <- trace_plot_row_counter + 1
}
new_imputed_dataset <- dataset_master_copy
}
imputed_datasets <- append(imputed_datasets, list(new_imputed_dataset))
}
object <- list(datasets = imputed_datasets, trace = trace_plot)
return(list(object))
}
nhanes_incomplete <- nhanes
completed_nhanes <- misl(nhanes_incomplete)
completed_nhanes
use_r("check_data_type")
getwd()
library('devtools')
use_r("check_data_type")
library(tidyverse)
?readr
?read_csv
getwd()
setwd("/Users/thomascarpenito/Documents/programming/kaggle/jan_tabular_playground/")
train_data <- read_csv("data/train.csv")
train_data
dim(train_data)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reshape2)
corr_matrix <- round(cor(train_data),2)
corr_matrix
train_data_no_id <- train_data %>%
select(-id)
corr_matrix <- round(cor(train_data_no_id),2)
corr_matrix
melted_corr_matrix <- melt(corr_matrix)
melted_corr_matrix
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile()
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = white)
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white")
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation")
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimial()
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_bw()
corr_matrix
?upper.tri()
tri_corr_matrix <- upper.tri(corr_matrix)
tri_corr_matrix
tri_corr_matrix <- corr_matrix[upper.tri(corr_matrix)]
tri_corr_matrix
# But for our correlation plot we need the data to be in a specific format:
melted_corr_matrix <- melt(corr_matrix)
melted_corr_matrix
melted_corr_matrix <- melt(tri_corr_matrix)
melted_corr_matrix
# First create the correlation matrix
corr_matrix <- round(cor(train_data_no_id),2)
# For our plot it would be nice to just have the upper triangle (since it's mirrored):
corr_matrix[upper.tri(corr_matrix)] <- NA
corr_matrix
# First create the correlation matrix
corr_matrix <- round(cor(train_data_no_id),2)
# For our plot it would be nice to just have the lower triangle (since it's mirrored):
corr_matrix[upper.tri(corr_matrix)] <- NA
# But for our correlation plot we need the data to be in a specific format:
melted_corr_matrix <- melt(corr_matrix)
melted_corr_matrix
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_bw()
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()
# But for our correlation plot we need the data to be in a specific format:
melted_corr_matrix <- melt(corr_matrix, na.rm = TRUE)
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_bw()
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme_bw()
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_hex(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme_bw()
?geom_tile
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme_bw()
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme(axis.text.x=element_text(angle=90, hjust=1))
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme(axis.text.x=element_text(angle=45, hjust=1))
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme(axis.text.x=element_text(angle=45, hjust=1), color("white"))
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme(axis.text.x=element_text(angle=45, hjust=1), panel.background = "white")
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme(axis.text.x=element_text(angle=45, hjust=1), panel.background = element_rect(color = "white"))
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme_tufte()
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme_bw()
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme_bw(axis.text.x=element_text(angle=45, hjust=1))
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
theme_bw()
# We are now ready for the geom_tile() function of ggplot():
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1)) +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1))
train_data_no_id
melted_data <- melt(train_data_no_id)
melted_data
View(melted_data)
ggplot(data = melted_data) +
geom_boxplot(aes(x = variable, y = value ))
ggplot(data = melted_data, aes(x = variable, y = value)) +
geom_boxplot()
ggplot(data = melted_data, aes(x = variable, y = value, color = variable)) +
geom_boxplot()
ggplot(data = melted_data, aes(x = variable, y = value, color = variable)) +
geom_boxplot() +
coord_flip()
ggplot(data = melted_data, aes(x = variable, y = value, color = variable)) +
geom_boxplot() +
coord_flip() +
theme_bw()
ggplot(data = melted_data, aes(x = variable, y = value, color = variable)) +
geom_boxplot() +
coord_flip() +
theme_bw() +
theme(legend.position = NULL)
ggplot(data = melted_data, aes(x = variable, y = value, color = variable)) +
geom_boxplot() +
coord_flip() +
theme_bw() +
theme(legend.position = "none")
install.packages("stacks")
library(stacks)
vignette(stacks)
library(stacks)
vignette(stacks)
?stacks
?initial_split()
library(tidyverse)
library(reshape2)
library(stacks)
library(tidymodels)
install.packages("tidymodels")
install.packages("tidymodels")
install.packages("tidymodels")
library(tidyverse)
library(reshape2)
library(stacks)
library(tidymodels)
library(tidyverse)
library(reshape2)
library(stacks)
library(tidymodels)
library(dplyr)
library(purrr)
?initial_split
train_data <- read_csv("data/train.csv")
training_split <- initial_split(train_data)
training_split
# First create an initial training and test split
kaggle_split <- initial_split(train_data)
kaggle_train <- training(kaggle_split)
kaggle_test <- testing(kaggle_split)
set.seed(123)
?recipe
View(hed(kaggle_train))
head(kaggle_train)
View(kaggle_train)
kaggle_recipe <-
recipe(target ~ ., data = kaggle_train)
metric <- metric_set(rmse)
ctrl_grid <- control_stack_grid()
ctrl_res <- control_stack_resamples()
?control_stack_grid
knn_spec <-
nearest_neighbor(
mode = "regression",
neighbors = tune("k")
) %>%
set_engine("kknn")
knn_spec
?set_engine
?step_dummy
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_zv(all_predictors(), skip = TRUE) %>%
step_meanimpute(all_numeric(), skip = TRUE) %>%
step_normalize(all_numeric(), skip = TRUE)
knn_rec
View(head(kaggle_train))
?step_rm
# We need to define the recipe for the steps each learner will take
kaggle_recipe <- recipe(target ~ . - id, data = kaggle_train)
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_rm(contains("id"))
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_rm(contains("id")) %>%
step_zv(all_predictors(), skip = TRUE) %>%
step_meanimpute(all_numeric(), skip = TRUE) %>%
step_normalize(all_numeric(), skip = TRUE)
knn_rec
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_rm(contains("cont")) %>%
step_zv(all_predictors(), skip = TRUE) %>%
step_meanimpute(all_numeric(), skip = TRUE) %>%
step_normalize(all_numeric(), skip = TRUE)
knn_rec
knn_rec
kaggle_recipe
# We need to define the recipe for the steps each learner will take
kaggle_recipe <- recipe(target ~ ., data = kaggle_train)
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_rm(contains("id")) %>%
step_zv(all_predictors(), skip = TRUE) %>%
step_meanimpute(all_numeric(), skip = TRUE) %>%
step_normalize(all_numeric(), skip = TRUE)
knn_rec
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_zv(all_predictors(), skip = TRUE) %>%
step_meanimpute(all_numeric(), skip = TRUE) %>%
step_normalize(all_numeric(), skip = TRUE)
knn_rec
?step_zv
?step_meanimpute
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_rm(contains("id")) %>%
step_meanimpute(all_numeric(), skip = TRUE) %>%
step_normalize(all_numeric(), skip = TRUE)
knn_rec
knn_rec <-
kaggle_recipe %>%
step_rm(contains("id"))
knn_rec
?step_rm
library(modeldata)
data(biomass)
biomass_tr <- biomass[biomass$dataset == "Training",]
biomass_te <- biomass[biomass$dataset == "Testing",]
rec <- recipe(HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur,
data = biomass_tr)
library(dplyr)
smaller_set <- rec %>%
step_rm(contains("gen"))
smaller_set
rec
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_rm(contains("id")) %>%
step_meanimpute(all_numeric(), skip = TRUE) %>%
step_normalize(all_numeric(), skip = TRUE)
knn_rec
?nmiss
kaggle_train %>%
mutate(missing_data = colSums(is.na))
colSums(train_data)
colSums(is.na(train_data))
# Begin the workflow
knn_wflow <-
workflow() %>%
add_model(knn_spec) %>%
add_recipe(knn_rec)
knn_wflow
# And now we are ready to apply it to our data:
knn_rec <-
kaggle_recipe %>%
step_rm(contains("id")) %>%
step_normalize(all_numeric(), skip = TRUE)
# Begin the workflow
knn_wflow <-
workflow() %>%
add_model(knn_spec) %>%
add_recipe(knn_rec)
knn_wflow
# Putting it all together
knn_res <-
tune_grid(
knn_wflow,
resamples = folds,
metrics = metric,
grid = 4,
control = ctrl_grid
)
folds <- rsample::vfold_cv(kaggle_train, v = 10)
# Putting it all together
knn_res <-
tune_grid(
knn_wflow,
resamples = folds,
metrics = metric,
grid = 4,
control = ctrl_grid
)
install.packages("kknn")
?kknn
# Putting it all together
knn_res <-
tune_grid(
knn_wflow,
resamples = folds,
metrics = metric,
grid = 4,
control = ctrl_grid
)
knn_res
