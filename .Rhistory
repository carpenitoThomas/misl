?predict
library('devtools')
load_all()
load("/Users/thomascarpenito/Desktop/gold_standard.Rdata")
dataset <- gold_standard
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial", "Lrnr_ranger", "Lrnr_svm")
ignore_predictors = c("STRATUM", "PSU", "WT_C")
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- colnames(dataset)[colSums(is.na(dataset))!=0]
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe.
# You can see in this intialize function: https://github.com/amices/mice/blob/46171f911af7c7c668b4bffc3976f5669436bafd/R/initialize.imp.R
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  impute_placeholders(dataset_master_copy, column_number, missing_default)
}
column <- "ARG"
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Because this is a bootstrap sample, we need to relevel the factor columns depending on what factors are included.
# If this is not done then it will crash the learners in SL3.
bootstrap_sample <- droplevels(bootstrap_sample)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
xvars
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations)
sl3_task <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
learners
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
# This bit of code needs some further thought... basically if the outcome is categorical and the bootstrap sample does NOT include the same number of levels
# as the original dataset (from the nature of sampling), then we need to re-level our data we are predicting on to match the categorical outcome...
# Again, this will need futher thought....
if(outcome_type == "categorical"){
dataset_master_copy[[yvar]] <- factor(dataset_master_copy[[yvar]], levels=levels(bootstrap_sample[[yvar]]))
}
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# But, we're going to follow the bootstrap matching method: https://stefvanbuuren.name/fimd/sec-cart.html#sec:cartoverview
# Which is interesting becuase it looks like our beta hat and beta dot are one in the same: https://stefvanbuuren.name/fimd/sec-categorical.html
predictions_task <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions <- sl_stack_fit$predict(predictions_task)
str(dataset_master_copy$ARG)
str(bootstrap_sample$ARG)
outcome_type
str(dataset_master_copy) == str(bootstrap_sample)
a <- str(dataset_master_copy)
a
apple <- str(dataset_master_copy)
apple[1]
str(dataset_master_copy)
str(bootstrap_sample)
table(bootstrap_sample$BDYPT)
str(bootstrap_sample$BDYPT)
str(dataset_master_copy$BDYPT)
table(dataset_master_copy$BDYPT)
predictions <- sl_stack_fit$predict(predictions_task)
xvars
sl_stack_fit$predict
?sl_stack_fit$predict
predictions_task
sl3_task
sl3_task$covariates
sl3_task
predictions_task
predictions <- sl_stack_fit$predict(predictions_task)
?lrnr_svm
str(dataset_master_copy)
str(bootstrap_sample)
is.factor(dataset_master_copy[[yvar]])
load_all()
build(vignettes = FALSE)
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Because this is a bootstrap sample, we need to relevel the factor columns depending on what factors are included.
# If this is not done then it will crash the learners in SL3.
bootstrap_sample <- droplevels(bootstrap_sample)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations)
sl3_task <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
# This bit of code needs some further thought... basically if any of the columns are categorical and the bootstrap sample does NOT include the same number of levels
# as the original dataset (from the nature of sampling), then we need to re-level our data we are predicting on to match the categorical column...
# Again, this will need futher thought....
for(column_number in seq_along(dataset_master_copy)){
if(is.factor(dataset_master_copy[[column_number]])){
dataset_master_copy[[column_number]] <- factor(dataset_master_copy[[column_number]], levels=levels(bootstrap_sample[[column_number]]))
}
}
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# But, we're going to follow the bootstrap matching method: https://stefvanbuuren.name/fimd/sec-cart.html#sec:cartoverview
# Which is interesting becuase it looks like our beta hat and beta dot are one in the same: https://stefvanbuuren.name/fimd/sec-categorical.html
predictions_task <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
View(dataset_master_copy)
colSums(is.na(dataset_master_copy))
predictions <- sl_stack_fit$predict(predictions_task)
# Here we can begin selection from a canidate donor
# Note, this is unclear... becuase we are using a technique like CART but we don't have terminal nodes.
# But also PMM didn't distinguish how one matches when using bootstrap?
# So, we're not going to be matching with binary or categorical variables, we can just use sampling from their distribution.
# https://stefvanbuuren.name/fimd/sec-categorical.html
if(outcome_type == "binomial"){
predicted_values <- stats::rbinom(length(dataset_master_copy[[column]]), 1, predictions)
dataset_master_copy[[column]] <- ifelse(is.na(dataset[[column]]), predicted_values, dataset[[column]])
}else if(outcome_type == "continuous"){
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),5)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[sample(distance,1)]
}
dataset_master_copy[[column]]<- ifelse(is.na(dataset[[column]]), list_of_matches, dataset[[column]])
}else if(outcome_type== "categorical"){
predicted_values <- Hmisc::rMultinom(sl3::unpack_predictions(predictions),1)
dataset_master_copy[[column]] <-  factor(ifelse(is.na(dataset[[column]]), predicted_values, as.character(dataset[[column]])), levels = levels(dataset[[column]]))
}
# Append to the trace plot only if a numeric column
if(outcome_type != "categorical" & sum(is.na(dataset[[column]])) > 0){
trace_plot$value[trace_plot$variable == column & trace_plot$m == m_loop & trace_plot$iteration == i_loop & trace_plot$statistic == "mean"] <- mean(dataset_master_copy[[column]][is.na(dataset[[column]])])
trace_plot$value[trace_plot$variable == column & trace_plot$m == m_loop & trace_plot$iteration == i_loop & trace_plot$statistic == "sd"] <- sd(dataset_master_copy[[column]][is.na(dataset[[column]])])
}
build(vignettes = FALSE)
View(dataset_master_copy)
colSums(is.na(dataset_master_copy))
# Here we can begin selection from a canidate donor
# Note, this is unclear... becuase we are using a technique like CART but we don't have terminal nodes.
# But also PMM didn't distinguish how one matches when using bootstrap?
# So, we're not going to be matching with binary or categorical variables, we can just use sampling from their distribution.
# https://stefvanbuuren.name/fimd/sec-categorical.html
if(outcome_type == "binomial"){
predicted_values <- stats::rbinom(length(dataset_master_copy[[column]]), 1, predictions)
dataset_master_copy[[column]] <- ifelse(is.na(dataset[[column]]), predicted_values, dataset[[column]])
}else if(outcome_type == "continuous"){
# If continuous, we can do matching
# Find the 5 closest donors and making a random draw from them
list_of_matches <- c()
for(value in seq_along(predictions)){
distance <- head(order(abs(predictions[value] - ifelse(is.na(dataset[[column]]), NA, predictions))),5)
list_of_matches[value] <- ifelse(is.na(dataset[[column]]), NA, dataset[[column]])[sample(distance,1)]
}
dataset_master_copy[[column]]<- ifelse(is.na(dataset[[column]]), list_of_matches, dataset[[column]])
}else if(outcome_type== "categorical"){
predicted_values <- Hmisc::rMultinom(sl3::unpack_predictions(predictions),1)
dataset_master_copy[[column]] <-  factor(ifelse(is.na(dataset[[column]]), predicted_values, as.character(dataset[[column]])), levels = levels(dataset[[column]]))
}
colSums(is.na(dataset_master_copy))
load("/Users/thomascarpenito/Desktop/gold_standard.Rdata")
dataset <- gold_standard
ignore_predictors = c("STRATUM", "PSU", "WT_C")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial", "Lrnr_ranger", "Lrnr_svm")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger")
con_method = c("Lrnr_mean", "Lrnr_glm_fast", "Lrnr_earth", "Lrnr_ranger")
# Initializes the trace plot (for inspection of imputations)
trace_plot <- expand.grid(statistic = c("mean", "sd"), value = NA, variable = colnames(dataset), m = m_loop, iteration = seq_along(1:maxit))
# Identifies which columns need to be imputed. According to van Buren, this order does not matter.
# https://stefvanbuuren.name/fimd/sec-algoptions.html
column_order <- colnames(dataset)[colSums(is.na(dataset))!=0]
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe.
# You can see in this intialize function: https://github.com/amices/mice/blob/46171f911af7c7c668b4bffc3976f5669436bafd/R/initialize.imp.R
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  impute_placeholders(dataset_master_copy, column_number, missing_default)
}
str(dataset)
column <- "BDYPT"
# First, we extract all complete records with respect to the column we are imputing.
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
levels(full_dataframe$BDYPT)
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
levels(bootstrap_sample$BDYPT)
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
levels(bootstrap_sample$BDYPT)
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
levels(bootstrap_sample)
levels(bootstrap_sample$BDYPT)
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
levels(bootstrap_sample$BDYPT)
table(bootstrap_sample$BDYPT, exclude = NULL)
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
table(bootstrap_sample$BDYPT, exclude = NULL)
table(dataset$BDYPT, exclude = NULL)
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
nrow(full_dataframe)
column
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
table(bootstrap_sample$BDYPT)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners.
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations)
sl3_task <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
?lrnr_solnp
?solnp
?Lrnr_solnp
sl_train
sl
sl3_task
levels(bootstrap_sample$BDYPT)
table(bootstrap_sample$BDYPT, exclude = NULL)
learners
learners <- c("Lrnr_mean", "Lrnr_independent_binomial", "Lrnr_ranger")
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial")
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
for(column_number in seq_along(bootstrap_sample)){
print(column_number)
}
re_assign_cat_learners <- FALSE
re_assign_cat_learners
for(column_number in seq_along(bootstrap_sample)){
temp_data <- droplevels(bootstrap_sample)
if(levels(temp_data[[column_number]]) != levels(bootstrap_sample[[column_number]])){
re_assign_cat_learners <- TRUE
}
}
seq_along(bootstrap_sample)
temp_data <- droplevels(bootstrap_sample)
temp_data
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
temp_data <- droplevels(bootstrap_sample)
if(is.factor(temp_data[[column_number]])){
if(levels(temp_data[[column_number]]) != levels(bootstrap_sample[[column_number]])){
re_assign_cat_learners <- TRUE
}
}
}
warnings()
colnames(bootstrap_sample)
column_number <- 2
temp_data <- droplevels(bootstrap_sample)
is.factor(temp_data[[column_number]]
)
levels(temp_data[[column_number]])
levels(bootstrap_sample[[column_number]])
column
column_number <- 7
is.factor(temp_data[[column_number]])
levels(temp_data[[column_number]])
levels(bootstrap_sample[[column_number]])
length(levels(temp_data[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))
re_assign_cat_learners <- FALSE
re_assign_cat_learners
for(column_number in seq_along(bootstrap_sample)){
temp_data <- droplevels(bootstrap_sample)
if(is.factor(temp_data[[column_number]])){
if(length(levels(temp_data[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
re_assign_cat_learners
?droplevels
boot_a <- bootstrap_sample
droplevels(boot_a)
levels(boot_a$BDYPT)
levels(temp_data)
levels(temp_data$BDYPT)
if(is.factor(temp_data[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
re_assign_cat_learners <- FALSE
re_assign_cat_learners
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(temp_data[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
re_assign_cat_learners
outcome_type
learners
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial", "Lrnr_ranger", "Lrnr_svm")
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
learner_list
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different array types. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(temp_data[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
learner_list
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model
sl_train <- sl3::delayed_learner_train(sl, sl3_task)
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched <- delayed::Scheduler$new(sl_train, delayed::FutureJob, verbose = FALSE)
sl_stack_fit <- sl_sched$compute()
# Again, this will need futher thought....
#for(column_number in seq_along(dataset_master_copy)){
#  if(is.factor(dataset_master_copy[[column_number]])){
#    dataset_master_copy[[column_number]] <- factor(dataset_master_copy[[column_number]], levels=levels(bootstrap_sample[[column_number]]))
#  }
#}
# Here we can create the predictions and then we can match them with the hot-deck method
# Interestingly, there are 4 different ways we can match: https://stefvanbuuren.name/fimd/sec-pmm.html#sec:pmmcomputation
# But, we're going to follow the bootstrap matching method: https://stefvanbuuren.name/fimd/sec-cart.html#sec:cartoverview
# Which is interesting becuase it looks like our beta hat and beta dot are one in the same: https://stefvanbuuren.name/fimd/sec-categorical.html
predictions_task <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions <- sl_stack_fit$predict(predictions_task)
build(vignettes = FALSE)
