View(post[, sampled_delta_col] / delta)
sampled_delta_col
View(post[, sampled_delta_col] / delta_cat)
delta_cat <- 2
View(post[, sampled_delta_col] / delta_cat)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
scale(post, center = FALSE, scale = rowSums(post))
scale(matrix(post), center = FALSE, scale = rowSums(matrix(post)))
View(post)
?scale
matrix_post <- as.matrix(post)
str(matrix_post)
scale(matrix_post, center = FALSE, scale = rowSums(matrix_post))
View(matrix_post)
scale(t(matrix_post), center = FALSE, scale = colSums(t(matrix_post)))
View(scale(t(matrix_post), center = FALSE, scale = colSums(t(matrix_post))))
View(t(scale(t(matrix_post), center = FALSE, scale = colSums(t(matrix_post)))))
.23605529  + 	0.28898132 + 0.47496339
post <- sl3::unpack_predictions(predictions_boot_dot)
t(scale(t(post), center = FALSE, scale = colSums(t(post))))
View(t(scale(t(post), center = FALSE, scale = colSums(t(post)))))
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
View(t(scale(t(post), center = FALSE, scale = colSums(t(post)))))
post <- sl3::unpack_predictions(predictions_boot_dot)
sampled_delta_col <- 2
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
View(post)
View(t(scale(t(post), center = FALSE, scale = colSums(t(post)))))
delta_cat <- 0
post <- sl3::unpack_predictions(predictions_boot_dot)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
View(post)
delta_cat <- 1
post <- sl3::unpack_predictions(predictions_boot_dot)
View(sl3::unpack_predictions(predictions_boot_dot))
post <- sl3::unpack_predictions(predictions_boot_dot)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
post <- t(scale(t(post), center = FALSE, scale = colSums(t(post))))
View(post)
delta_cat <- 2
post <- sl3::unpack_predictions(predictions_boot_dot)
post[, sampled_delta_col] <- post[, sampled_delta_col] / delta_cat
post <- t(scale(t(post), center = FALSE, scale = colSums(t(post))))
rowSums(post)
column_order
column <- "Older_12"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
yvar
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
outcome_type
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
View(predictions_boot_dot)
delta_cat
View(predictions_boot_dot / delta_cat)
column_order
column <- "Height"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
yvar
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
outcome_type
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
# This step only needs to compute if the outcome is continuous, saving some time
if(outcome_type == "continuous"){
sl_train_full_hat <- sl3::delayed_learner_train(sl, sl3_task_full_hat)
sl_sched_full_hat <- delayed::Scheduler$new(sl_train_full_hat, delayed::FutureJob)
sl_stack_fit_full_hat <- sl_sched_full_hat$compute()
predictions_task_full_hat <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_full_hat <- sl_stack_fit_full_hat$predict(predictions_task_full_hat)
}
View(predictions_boot_dot)
hist(abalone$Height)
delta_con
delta_con <- 0
# We can add a bit of augemntation here for the sensitivity analysis
# By default, this should not affect results as we will be adding 0, otherwise, augment the imputations
predictions_boot_dot <- predictions_boot_dot + delta_con
build(vignettes = FALSE)
load_all()
?misl
load_all()
?misl
build(vignettes = FALSE)
load_all()
library(devtools)
library("devtools")
load_all()
?misl
library('devtools')
install_github("carpenitoThomas/misl", auth_token = "539e593f0a5ef03697a9592f367b7b3cba79fd61")
library("misl")
?misl
document()
install_github("carpenitoThomas/misl", auth_token = "539e593f0a5ef03697a9592f367b7b3cba79fd61")
library("misl")
?`missing-class`
?misl
a <- NA
if(a == "TOMMY"){print("HI")}
if(a == "TOMMY"){print("HI")
}
a
b <- "BOB"
if(b == "TOMMY"){print("HI")}
column <- "test"
if(column == a){print("TOmmy")}
load_all()
library('devtools')
load_all()
dataset <- abalone
misl(abalone, maxit = 2, m = 2, quiet = FALSE, cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"))
misl(abalone, maxit = 2, m = 1, quiet = FALSE, cat_method = c("Lrnr_independent_binomial", "Lrnr_mean"), delta_cat = 3, delta_var = "Sex")
build()
build(vignettes = FALSE)
document()
build_all()
library('devtools')
build(vignettes = FALSE)
library('devtools')
load_all()
dataset <- abalone
m = 5
maxit = 5
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial")
ignore_predictors = NA
quiet = FALSE
delta_cat <- 2
delta_var <- "Sex"
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
column_order
column <- "Sex"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
delta_adj
# This is a quick fix for the sensitivity analysis that will require further thought
# Essentially, we are chceking to see if this is the value we need to augment or not.
# If it's not, then no changes happen
delta_adj = FALSE
if(!is.na(delta_var)){
if(column == delta_var){
delta_adj = TRUE
}
}
delta_adj
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_task_boot_dot
predictions_task_boot_dot$weights
predictions_task_boot_dot$offset
View(predictions_task_boot_dot$weights)
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
View(predictions_boot_dot)
sl_stack_fit_boot_dot
predictions_task_boot_dot
?predict
str(predictions_task_boot_dot)
predictions_task_boot_dot$predict
predictions_task_boot_dot$predict(tommy)
predictions_task_boot_dot$predict("tommy")
sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
predictions_task_boot_dot$data
View(predictions_task_boot_dot$data)
View(predictions_task_boot_dot$offset)
View(predictions_task_boot_dot$X)
View(predictions_task_boot_dot$Y)
View(predictions_task_boot_dot$predictions_boot_dot)
predictions_boot_dot
View(sl3::unpack_predictions(predictions_boot_dot))
View(sl3::print.packed_predictions(predictions_boot_dot))
View(sl3::unpack_predictions(predictions_boot_dot))
print(unlist(print.packed_predictions))
print(unlist(predictions_boot_dot))
View(unlist(predictions_boot_dot))
predictions_task_boot_dot
View(predictions_task_boot_dot)
predictions_task_boot_dot
predictions_task_boot_dot$folds
predictions_task_boot_dot$nodes
predictions_task_boot_dot$X
View(predictions_task_boot_dot$X)
View(predictions_task_boot_dot$Y)
View(predictions_task_boot_dot$X_intercept)
View(abalone)
View(predictions_task_boot_dot$weights)
View(predictions_task_boot_dot$internal_data)
View(predictions_task_boot_dot$uuid)
View(predictions_task_boot_dot$get_data())
View(predictions_task_boot_dot$subset_task()
)
View(predictions_task_boot_dot$time)
sl_stack_fit_boot_dot$predict
?sl_stack_fit_boot_dot$predict
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
predictions_task_boot_dot
sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
sl_stack_fit_boot_dot$predict(predictions_task_boot_dot, prob = FALSE)
predictions_task_boot_dot
predictions_task_boot_dot$outcome_type
predictions_task_boot_dot$.__enclos_env__
predictions_task_boot_dot$Y
predictions_task_boot_dot$folds
predictions_task_boot_dot$subset_task()
predictions_task_boot_dot$Y
temp
sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
predictions_boot_dot[1]
sum(predictions_boot_dot[1])
library('devtools')
load_all()
dataset <- abalone
m = 5
maxit = 5
seed = NA
con_method = c("Lrnr_mean", "Lrnr_glm_fast")
bin_method = c("Lrnr_mean", "Lrnr_glm_fast")
cat_method = c("Lrnr_mean", "Lrnr_independent_binomial")
ignore_predictors = NA
quiet = FALSE
delta_cat = 1
delta_var = "Sex"
# Identifies which columns need to be imputed. According to van Buren, this order does not matter
# https://stefvanbuuren.name/fimd/sec-algoptions.html
# Future work should explore if this makes a difference
column_order <- sample(colnames(dataset)[colSums(is.na(dataset))!=0])
# Retain a copy of the dataset for each of the new m datasets
dataset_master_copy <- dataset
# As with all gibbs sampling methods, we will need to initialize the starting dataframe
# This is step 2 of https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice
for(column_number in seq_along(dataset_master_copy)){
dataset_master_copy[is.na(dataset_master_copy[[column_number]]), column_number] <-  sample(dataset[[column_number]][!is.na(dataset[[column_number]])], sum(is.na(dataset[[column_number]])), replace = TRUE)
}
column_order
column <- Sex
column <- "Sex"
# First, we extract all complete records with respect to the column we are imputing
# This is our y_dot_obs and x_dot_obs
# https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
full_dataframe <- dataset_master_copy[!is.na(dataset[[column]]), ]
# This is a quick fix for the sensitivity analysis that will require further thought
# Essentially, we are chceking to see if this is the value we need to augment or not.
# If it's not, then no changes happen
delta_adj = FALSE
if(!is.na(delta_var)){
if(column == delta_var){
delta_adj = TRUE
}
}
delta_adj
delta_var
# To avoid complications with variance estimates of the ensemble, we will use bootstrapping
# See note below algorithm: https://stefvanbuuren.name/fimd/sec-pmm.html#def:pmm
# We can also see the following: https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:normboot
# Note, for this method we still need to calculate beta_hat and beta_dot, unfortunately this means super learner twice
bootstrap_sample <- dplyr::sample_n(full_dataframe, size = nrow(full_dataframe), replace = TRUE)
# Next identify the predictors (xvars) and outcome (yvar) depending on the column imputing
xvars <- colnames(bootstrap_sample[ , -which(names(bootstrap_sample) %in% c(column)), drop = FALSE])
if(!is.na(ignore_predictors[1])){
xvars <- xvars[-which(xvars %in% ignore_predictors)]
}
yvar <- column
# Specifying the outcome_type will be helpful for checking learners
outcome_type <- check_datatype(dataset[[yvar]])
# First, define the task using our bootstrap_sample (this helps with variability in imputations) and our full_dataframe sample
sl3_task_boot_dot <- sl3::make_sl3_Task(bootstrap_sample, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
sl3_task_full_hat <- sl3::make_sl3_Task(full_dataframe, covariates = xvars, outcome = yvar, outcome_type = outcome_type )
# Depending on the outcome, we need to build out the learners
learners <- switch(outcome_type,
categorical = cat_method,
binomial = bin_method ,
continuous = con_method)
# If after drawing a bootstrap sample, any of the columns DO NOT contain the same factors as in the original data, then the algorithm will fail
# This is set up by design becuase the super learner cannot make out of sample predictions and the meta-learner will not know how
# to deal with the different factor levels. Should this happen, we will print a message to the user letting them know that the machine learning algorithms could NOT be used
# in this instance and instead for this iteration they must rely on the mean and a series of independent binomial samples. This will be updated should more learners become available.
if(outcome_type == "categorical"){
re_assign_cat_learners <- FALSE
for(column_number in seq_along(bootstrap_sample)){
if(is.factor(bootstrap_sample[[column_number]])){
if(length(levels(droplevels(bootstrap_sample)[[column_number]])) != length(levels(bootstrap_sample[[column_number]]))){
re_assign_cat_learners <- TRUE
}
}
}
if(re_assign_cat_learners){
warning("Factor levels are not compatible between bootstrap and original dataframes. This occurs as a product of bootstrap sampling. Lrnr_mean and Lrnr_independent_binomial have been subsituted for this iteration.")
learners <- c("Lrnr_mean", "Lrnr_independent_binomial")
}
}
# Next, iterate through each of the supplied learners to build the SL3 learner list
learner_list <- c()
for(learner in learners){
code.lm <- paste(learner, " <- sl3::", learner, "$new()", sep="")
eval(parse(text=code.lm))
learner_list <- c(learner, learner_list)
}
# Next we stack the learners
learner_stack_code <- paste("stack", " <- sl3::make_learner(sl3::Stack,",paste(learner_list, collapse = ", "), ")", sep="")
eval(parse(text=learner_stack_code))
# Then we make and train the Super Learner
sl <- sl3::Lrnr_sl$new(learners = stack)
# We can then go ahead and train our model on the bootstrap data
sl_train_boot_dot <- sl3::delayed_learner_train(sl, sl3_task_boot_dot)
# We can finally execute the super learner
# This bit of code can be used if people wanted multi-threading (depending on computer capacity)
sl_sched_boot_dot <- delayed::Scheduler$new(sl_train_boot_dot, delayed::FutureJob)
sl_stack_fit_boot_dot <- sl_sched_boot_dot$compute()
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = .2 )
dataset$offset <- .2
dataset_master_copy$offset <- .2
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = offset )
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = "offset" )
predictions_task_boot_dot
dataset_master_copy$offset <- 100
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type, offset = "offset" )
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
post <- sl3::unpack_predictions(predictions_boot_dot)
View(post)
predictions_task_boot_dot <- sl3::sl3_Task$new(dataset_master_copy, covariates = xvars, outcome = yvar, outcome_type = outcome_type)
predictions_boot_dot <- sl_stack_fit_boot_dot$predict(predictions_task_boot_dot)
View(sl3::unpack_predictions(predictions_boot_dot))
?glm
